{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ALCF AI Testbed","text":"<p>The ALCF AI Testbed houses some of the most advanced AI accelerators for scientific research. </p> <p>The goal of the testbed is to enable explorations into next-generation machine learning applications and workloads, enabling the ALCF and its user community to help define the role of AI accelerators in scientific computing and how to best integrate such technologies with supercomputing resources.</p> <p>The AI accelerators complement the ALCF's current and next-generation supercomputers to provide a state-of-the-art computing environment that supports pioneering research at the intersection of AI, big data, and high performance computing (HPC). </p> <p>The platforms are equipped with architectural features that support AI and data-centric workloads, making them well suited for research tasks involving the growing deluge of scientific data produced by powerful tools, such as supercomputers, light sources, telescopes, particle accelerators, and sensors. In addition, the testbed will allow researchers to explore novel workflows that combine AI methods with simulation and experimental science to accelerate the pace of discovery.</p>"},{"location":"#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the AI Testbed\u2019s <code>Cerebras CS-2</code> and <code>SambaNova DataScale</code> platforms can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Access to additional testbed resources, including <code>Graphcore</code>, <code>Groq</code>, and <code>Habana</code> accelerators, will be announced at a later date. </p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li> <p>Request a Director's Discretionary project on SambaNova/Cerebras.</p> </li> <li> <p>Apply for an ALCF account after the project request is approved. Choose the SambaNova/Cerebras project that your PI has created at ALCF. If you have an active ALCF account, request to join the project after your project is approved.</p> </li> <li> <p>Transfer data to ALCF using Globus after your account has been created.</p> <p>a. The endpoint for your data in ALCF is <code>alcf#ai_testbed_projects</code> with the path to your project being  <code>/&lt;project name&gt;</code>. </p> <p>b. The endpoint for your home directory on the AI Testbeds in ALCF is <code>alcf#ai_testbed_home</code>.</p> </li> <li> <p>Add/invite team members to your ALCF project on SambaNova/Cerebras. </p> </li> </ol>"},{"location":"#how-to-contribute-to-documentation","title":"How to Contribute to Documentation","text":"<p>The documentation is based on MkDocs and source files are on GitHub. You can contribute to the documentation by creating a pull request. </p> <p>Learn more on how to contribute to documentation.</p>"},{"location":"HOWTO_CONTRIBUTE/","title":"AI Testbed User Guide","text":""},{"location":"HOWTO_CONTRIBUTE/#contributing-to-documentation","title":"Contributing to Documentation","text":""},{"location":"HOWTO_CONTRIBUTE/#python-environment","title":"Python environment","text":"<p>To build documentation locally, you need a Python environment with <code>mkdocs</code> installed.  Check that Python 3.6+ is installed:</p> <pre><code>$ python --version\nPython 3.8.3\n</code></pre> <p>Then create a new virtual env to isolate the <code>mkdocs</code> installation: <pre><code>$ python -m venv env\n$ source env/bin/activate\n</code></pre></p>"},{"location":"HOWTO_CONTRIBUTE/#git","title":"Git","text":"<p>Using Git ssh. Make sure you add ssh public key to your profile.</p> <p>Https cloning can be used with a Personal Access Token.</p> <pre><code>$ git clone git@github.com:argonne-lcf/ai-testbed-userdocs.git\n</code></pre>"},{"location":"HOWTO_CONTRIBUTE/#installing-mkdocs","title":"Installing Mkdocs","text":"<p>To install <code>mkdocs</code> in the current environment: </p> <pre><code>$ cd ai-testbed-userdocs\n$ make install-dev\n</code></pre>"},{"location":"HOWTO_CONTRIBUTE/#preview-the-docs-locally","title":"Preview the Docs Locally","text":"<p>This launches a server.  Do this in a seperate terminal.</p> <p>Run <code>mkdocs serve</code> or <code>make serve</code> to auto-build and serve the docs for preview in your web browser.</p> <pre><code>$ make serve\n</code></pre>"},{"location":"HOWTO_CONTRIBUTE/#working-on-documentation","title":"Working on documentation","text":"<ul> <li>All commits must have the commit comment</li> <li>Create your own branch from the main branch.  For this writing we are using YOURBRANCH as an example.</li> </ul> <p><pre><code>$ cd ai-testbed-userdocs\n$ git fetch --all\n$ git checkout main\n$ git pull origin main\n$ git checkout -b YOURBRANCH\n$ git push -u origin YOURBRANCH\n</code></pre> * Commit your changes to the remote repo <pre><code>$ cd ai-testbed-userdocs\n$ git status                         # check the status of the files you have editted\n$ git commit -a -m \"Updated docs\"    # preferably one issue per commit\n$ git status                         # should say working tree clean\n$ git push origin YOURBRANCH         # push YOURBRANCH to origin\n$ git checkout main                  # move to the local main\n$ git pull origin main               # pull the remote main to your local machine\n$ git checkout YOURBRANCH            # move back to your local branch\n$ git merge main                     # merge the local develop into **YOURBRANCH** and\n                                     # make sure NO merge conflicts exist\n$ git push origin YOURBRANCH         # push the changes from local branch up to your remote branch\n</code></pre> * Create pull request from https://github.com/argonne-lcf/ai-testbed-userdocs from YOURBRANCH to main branch.</p>"},{"location":"cerebras/Connect-to-a-CS-2-Node/","title":"Getting Started","text":""},{"location":"cerebras/Connect-to-a-CS-2-Node/#getting-started","title":"Getting Started","text":""},{"location":"cerebras/Connect-to-a-CS-2-Node/#connection-to-a-cs-2-node","title":"Connection to a CS-2 node","text":"<p> Connection to a CS-2 node is a two step process.  The first step requires a MFA passcode for authentication - either a 8 digit passcode generated by an app on your mobile device (e.g. mobilePASS+) or a CRYPTOCard-generated passcode prefixed by a 4 digit pin. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley. In the examples below, replace ALCFUserID with your ALCF user id.</p> <p>To connect to a CS-2 (\"chief\") node:</p> <ol> <li>From the local machine, ssh to the login node first:      <pre><code>ssh ALCFUserID@cerebras.alcf.anl.gov\n</code></pre></li> <li>From the login node, ssh to the destination CS-2 chief node:     <pre><code>ssh cs2-01-master\n</code></pre>     or     <pre><code>ssh cs2-02-master\n</code></pre></li> </ol> <p>Alternatively, this maybe done in one command line from the local machine. (two passcodes required): <pre><code>ssh -o \"ProxyJump ALCFUserID@cerebras.alcf.anl.gov\" ALCFUserID@cs2-01-master\n</code></pre> or <pre><code>ssh -o \"ProxyJump ALCFUserID@cerebras.alcf.anl.gov\" ALCFUserID@cs2-02-master\n</code></pre></p> <p>Verify that the connection was successful with <pre><code>uname -a\n</code></pre> and by making sure the output message contains <code>testbed-cs2-01-med1</code> or <code>testbed-cs2-02-med8</code>, and not <code>cs-login</code>.</p>"},{"location":"cerebras/Example-Programs/","title":"Example Programs","text":""},{"location":"cerebras/Example-Programs/#use-a-local-copy-of-the-model-zoo","title":"Use a local copy of the model zoo","text":"<p>Make a local copy of the Cerebras modelzoo and anl_shared repository, if not previously done, as follows.</p> <pre><code>mkdir ~/R1.5\ncp -r /software/cerebras/model_zoo/modelzoo/ ~/R1.5/modelzoo\ncp -r /software/cerebras/model_zoo/anl_shared/ ~/R1.5/anl_shared\n</code></pre>"},{"location":"cerebras/Example-Programs/#unet","title":"Unet","text":"<p>An implementation of this: U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger et.  al 2015 To run Unet with the Severstal: Steel Defect Detection kaggle dataset, using a pre-downloaded copy of the dataset,</p> <pre><code>cd ~/R1.5/modelzoo/unet/tf\n#rm -r model_dir_unet_base_severstal\ncp /software/cerebras/dataset/severstal-steel-defect-detection/params_severstal_sharedds.yaml configs/params_severstal_sharedds.yaml\ncsrun_cpu python run.py --mode=train --compile_only --params configs/params_severstal_sharedds.yaml --model_dir model_dir_unet_base_severstal --cs_ip $CS_IP\ncsrun_wse python run.py --mode=train --params configs/params_severstal_sharedds.yaml --model_dir model_dir_unet_base_severstal --max_steps 2000 --cs_ip $CS_IP\n</code></pre>"},{"location":"cerebras/Example-Programs/#bert","title":"Bert","text":"<p>An implementation of this: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding This BERT-large msl128 example uses a single sample dataset for both training and evaluation. See the README.md in the source directory for details on how to build a dataset from text input. <pre><code>cd ~/R1.5/modelzoo/transformers/tf/bert\ncp /software/cerebras/dataset/bert_large/params_bert_large_msl128_sampleds.yaml configs/params_bert_large_msl128_sampleds.yaml\n#rm -r model_dir_bert_large_msl128\ncsrun_cpu python run.py --mode=train --compile_only --params configs/params_bert_large_msl128_sampleds.yaml --model_dir model_dir_bert_large_msl128 --cs_ip $CS_IP\ncsrun_wse python run.py --mode=train --params configs/params_bert_large_msl128_sampleds.yaml --model_dir model_dir_bert_large_msl128 --cs_ip $CS_IP\n</code></pre></p>"},{"location":"cerebras/Example-Programs/#braggnn","title":"BraggNN","text":"<p>An implementation of this: BraggNN: fast X-ray Bragg peak analysis using deep learning The BraggNN model has two versions: 1) Convolution only - this version does not include the non-local attention block 2) Nonlocal - This version includes the nonlocal attention block as described in   https://arxiv.org/pdf/1711.07971.pdf</p> <pre><code>cd ~/R1.5/anl_shared/braggnn/tf\ncp /software/cerebras/dataset/BraggN/params_bragg_nonlocal_sampleds.yaml configs/params_bragg_nonlocal_sampleds.yaml\n#rm -r model_dir_braggnn\ncsrun_cpu python run.py -p configs/params_bragg_nonlocal_sampleds.yaml --model_dir model_dir_braggnn --mode train --compile_only --multireplica --cs_ip $CS_IP\ncsrun_wse python run.py -p configs/params_bragg_nonlocal_sampleds.yaml --model_dir model_dir_braggnn --mode train --multireplica --cs_ip $CS_IP\n</code></pre>"},{"location":"cerebras/Job-Queuing-and-Submission/","title":"Job Queuing and Submission","text":""},{"location":"cerebras/Job-Queuing-and-Submission/#job-queuing-and-submission","title":"Job Queuing and Submission","text":"<p>The CS-2 systems use slurm for job submission and queueing. <code>csrun_cpu</code> is used to run a cpu-only job on one or more worker nodes. <code>csrun_wse</code> is used to run a job on both the wafer scale engine and one or more worker nodes.</p> <p>Your job will be blocked until there are available resources. Scheduling is in first in, first out (FIFO) order. <pre><code># csrun_cpu [--help] [--alloc_node] [--mount_dirs] command_to_execute\ncsrun_cpu --help\n# csrun_wse [--help] [--total-nodes] [--tasks_per_node] [--cpus_per_task] [--mount_dirs] command_for_cs_execution\ncsrun_wse --help\n#\n# squeue is used to inspect the job queue\n# squeue [OPTIONS]\nsqueue -a\n# scancel is used to cleanly kill a job\n# scancel [OPTIONS] [job_id[_array_id][.step_id]]\nscancel JOBID\n</code></pre></p> <p>Note: slurm jobs using the wafer (started using <code>csrun_wse</code>) will show with two parts in the <code>squeue -a</code> output. The jobids will be the base ID plus 0 or 1, e.g. 555+0 and 555+1. To cancel using <code>scancel</code>, give just the base ID as the argument, e.g. <code>scancel 555</code>. </p> <p>You can find a detailed documentation for slurm here or use <code>--help</code> to see a summary of options for slurm commands, i.e. <code>squeue --help</code>. See some examples of how these commands are used to submit and queue jobs in section Steps to run a model/program.</p>"},{"location":"cerebras/Miscellaneous/","title":"Miscellaneous","text":""},{"location":"cerebras/Miscellaneous/#porting-applications-to-the-cs-2","title":"Porting applications to the CS-2","text":"<p>Cerebras\u2019s mature Python support is built around Cerebras Estimator, which inherits from TensorFlow Estimator. A Keras model can be converted to TF Estimator, and hence to a Cerebras Estimator. See https://www.tensorflow.org/tutorials/estimator/keras_model_to_estimator for more information on conversion of Keras models.</p> <p>Cerebras has recently introduced PyTorch support. The PyTorch support is documented at Cerebras Software Documentation in the section DEVELOP WITH PYTORCH. </p> <p>Cerebras has guides for porting TensorFlow and PyTorch models: Port TensorFlow to Cerebras Porting PyTorch Model to CS This is Cerebras's list of the TensorFlow layers that they support (for the current version): Supported TensorFlow Layers This is Cerebras's list of the PyTorch operations supported (for the current version): Supported PyTorch Ops</p> <p>When porting, it is often helpful to study a related example in the Cerebras modelzoo. A copy of the modelzoo for the install release is at <code>/software/cerebras/model_zoo/modelzoo/</code> Both the <code>README.md</code> files and source code in the modelzoo can be quite helpful.</p>"},{"location":"cerebras/Miscellaneous/#determining-the-cs-2-version","title":"Determining the CS-2 version","text":"<p>These queries will only work on cs2-01 due to networking constraints: <pre><code>...$ # Query the firmware level for cs2-01\n...$ curl -k -X GET 'https://192.168.120.30/redfish/v1/Managers/manager' --header 'Authorization: Basic YWRtaW46YWRtaW4=' 2&gt; /dev/null  | python -m json.tool | grep FirmwareVersion\n\"FirmwareVersion\": \"1.1.1-202203171919-5-879ff4ef\",\n...$\n\n...$ # Query the firmware level for cs2-02 (from cs2-01)\n...$ curl -k -X GET 'https://192.168.120.50/redfish/v1/Managers/manager' --header 'Authorization: Basic YWRtaW46YWRtaW4=' 2&gt; /dev/null  | python -m json.tool | grep FirmwareVersion\n\"FirmwareVersion\": \"1.1.1-202203171919-5-879ff4ef\",\n...$\n</code></pre></p>"},{"location":"cerebras/Miscellaneous/#copying-files","title":"Copying files","text":"<p>To copy a file to your CS-2 home dir (same on both CS2 clusters), replacing both instances of ALCFUserID with your ALCF user id: <pre><code>scp -o \"ProxyJump ALCFUserID@cerebras.alcf.anl.gov\" filename ALCFUserID@cs2-01-master:~/\n</code></pre></p> <p>To copy a file from your CS-2 home dir (same on both CS2 clusters) to the current local directory, replacing both instances of ALCFUserID with your ALCF user id: <pre><code>scp -o \"ProxyJump ALCFUserID@cerebras.alcf.anl.gov\" ALCFUserID@cs2-01-master:~/filename .\n</code></pre></p>"},{"location":"cerebras/Miscellaneous/#downloading-a-kaggle-competition-dataset-to-a-cs-2-node-using-the-command-line","title":"Downloading a Kaggle competition dataset to a CS-2 node using the command line","text":"<p>These notes may be helpful for downloading some Kaggle datasets</p> <p>Inside a singularity shell (e.g. <code>singularity shell -B /opt:/opt /software/cerebras/cs2-02/container/cbcore_latest.sif</code> )</p> <pre><code>virtualenv env\nsource env/bin/activate\npip3 install kaggle\n</code></pre> <p>Go to www.kaggle.com in a browser, log in (create account if first time). In user(icon upper right) -&gt; Account tab, there is a button (scroll down) to \"Create New API Token\". Click it. It will open a download window for a one line json. </p> <p>put the json in <code>~/.kaggle/kaggle.json</code> e.g. scp the downloaded file, or single quote the json text and echo it as shown <pre><code>mkdir ~/.kaggle\necho '{\"username\":\"REDACTED\",\"key\":\"REDACTED\"}' &gt; ~/.kaggle/kaggle.json\nchmod 600 ~/.kaggle/kaggle.json\n</code></pre></p> <p>On www.kaggle.com, the kaggle api command for download of a dataset is displayed in the data tab. It can be selected and copied to the local clipboard, or copied with the \"Copy API command to clipboard\" icon. Before attempting a download, if there is a button on the kaggle download page to agree to any terms and conditions, e.g. agreement to the competition rules, click on it (after reading them); downloads with your access token will fail with a 403 error until you agree to those T&amp;Cs.</p> <p>Paste the API command to the command line inside the singularity shell with the venv activated. E.g. <pre><code>kaggle datasets download -d mhskjelvareid/dagm-2007-competition-dataset-optical-inspection\n</code></pre></p> <p>It will download as a zip file. </p> <p>Exit the singularity container (with <code>exit</code>), then unzip the dataset zip file. <code>unzip</code> is available on the CS2 worker nodes.</p> <p>Note: the kaggle download shown above included two identical copies of the dataset; one copy was in a subdirectory.</p>"},{"location":"cerebras/Performance-Tools/","title":"Performance Tools","text":""},{"location":"cerebras/Performance-Tools/#compile-report","title":"Compile Report","text":"<p>After a compile, see the generated compile_report.txt. \"Active PE's\" is the percentage of processing elements (PEs) on the wafer that will be used by the model (compute + transmission). \"Compute Utilization\" is the estimated percentage of time these active PEs will be running the kernel code. The product of \"Active PEs\" and \"Compute Utilization\" is the effective wafer utilization as estimated by the compiler when the application is not I/O bound. Note: these two percentages are rounded to the nearest percent in the compile_report.txt, and e.g. can be displayed as \"0%\". </p> <pre><code>...$ find . -name \"compile_report.txt\" -exec head -n 7 {} \\;\nEstimated Overall Performance\n-------------------------------\nSamples/s:                    14724.2\nCompute Samples/s:            17287.7\nTransmission Samples/s:       14724.2\nActive PEs:                   62%\nCompute Utilization:          52%\n...$\n</code></pre> <p>In this example, the wafer utilization estimate is 32 percent (0.62*0.52). There is no sharing of the wafer by different jobs, so Cerebras users should strive to maximize their jobs' use of the wafer.  For more details, see the Cerebras documentation: Compile Report Consider using multiple model replicas if the model is only filling a small part of the wafer. See https://docs.cerebras.net/en/latest/tensorflow-docs/multiple-models/multi-replica-data-parallel-training.html For code samples with multireplica support, see <code>/software/cerebras/model_zoo/anl_shared/braggnn/tf/</code> and <code>/software/cerebras/model_zoo/modelzoo/fc_mnist/tf/</code>.</p>"},{"location":"cerebras/Performance-Tools/#cerebrass-guidance-on-sharding-and-shuffling-datasets","title":"Cerebras's guidance on sharding and shuffling datasets","text":"<p>This Cerebras document covers dataset sharding, and how to shuffle datasets. https://docs.cerebras.net/en/latest/tensorflow-docs/tuning-tf-for-cs/best-practices-tf.html</p>"},{"location":"cerebras/Performance-Tools/#cerebrass-guidance-on-the-compiler-console-output","title":"Cerebras's guidance on the compiler console output","text":"<p>This covers output to the console (and only to the console) during compile. Search the compile console output for any WARNING lines with the substring \"input_fn\". https://docs.cerebras.net/en/latest/compiler-reports/input-function-report.html</p>"},{"location":"cerebras/Performance-Tools/#cerebras-input-analyzer","title":"Cerebra's Input Analyzer","text":"<p>In version 1.2, Cerebras introduced the <code>cs_input_analyzer</code> script, which compiles the code, analyses the input pipeline, then suggests a slurm configuration and estimates the input performance.  https://docs.cerebras.net/en/latest/scripts-and-templates/cs-input-analyzer.html</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/","title":"Steps to Run a Model/Program","text":""},{"location":"cerebras/Steps-to-run-a-model-or-program/#getting-started","title":"Getting Started","text":"<p>[This subsection is an adaption of  https://docs.cerebras.net/en/latest/getting-started/checklist-before-you-start.html]</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#slurm","title":"Slurm:","text":"<p>Slurm is installed and running on all the CPU nodes. The coordination between a Cerebras system and the nodes in a Cerebras cluster is performed by Slurm. See section Job Queueing and Submission for more details.</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#worker-hostnames","title":"Worker hostnames:","text":"<p>The worker nodes (see the first diagram in System Overview) for the cs2-01 cluster are cs2-01-med[2-9]. The worker nodes (see the first diagram in System Overview) for the cs2-02 cluster are cs2-02-med[1-7]. You may occasionally need to log into a specific worker node for debugging purposes.</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#cs_ip-address-of-the-cerebras-system","title":"CS_IP address of the Cerebras system:","text":"<p>The CS-2 systems can be accessed using the <code>CS_IP</code> environment variable. This is set automatically on login. The CS_IP for cs2-01 is <code>192.168.220.30</code> The CS_IP for cs2-02 is <code>192.168.220.50</code></p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#running-slurm-jobs","title":"Running slurm jobs:","text":"<p>Cerebras includes two scripts for running slurm jobs. <code>csrun_cpu</code> is for running a Cerebras compilation. By default it reserves a single entire worker node. <code>csrun_wse</code> is for running a job on the wafer scale engine. By default it reserves five entire worker nodes, which are used to feed the dataset to the CS2 wafer. <code>csrun_cpu --help</code> and <code>csrun_wse --help</code> will list the available options. See section Job Queuing and Submission for more details.</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#execution-mode","title":"Execution mode:","text":"<p>The cs2 system supports two modes of execution. 1. Pipeline mode (default mode) Both cs2-01 and cs2-02 are currently configured for pipelined mode. This mode has more mature software support when compared to the weight streaming mode. 2. Weight streaming mode.(See the Weight Streaming Quickstart.) Weight streaming mode uses the host memory of one or more dedicated worker nodes to store model weights, and supports larger models compared to pipelined mode. Weight streaming mode is newly introduced in Rel 1.5, and supports only a  limited number of model layers.</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#running-a-training-job-on-the-wafer","title":"Running a training job on the wafer","text":"<p>Follow these instructions to compile and train the <code>fc_mnist</code> TensorFlow estimator example. This model is a couple of fully connected layers plus dropout and RELU. </p> <pre><code>cd ~/\nmkdir ~/R1.5/\ncp -r /software/cerebras/model_zoo/modelzoo ~/R1.5/modelzoo\ncd ~/R1.5/modelzoo/fc_mnist/tf\ncsrun_wse python run.py --mode train --cs_ip $CS_IP --max_steps 100000\n</code></pre> <p>You should see a training rate of about 1870 steps per second, and output that finishes with something similar to this:</p> <pre><code>INFO:tensorflow:Training finished with 25600000 samples in 53.424 seconds, 479188.55 samples/second.\nINFO:tensorflow:Loss for final step: 0.0.\n</code></pre> <p>To separately compile and train,</p> <pre><code># delete any existing compile artifacts and checkpoints\nrm -r model_dir\ncsrun_cpu python run.py --mode train --compile_only --cs_ip $CS_IP\ncsrun_wse python run.py --mode train --cs_ip $CS_IP --max_steps 100000\n</code></pre> <p>The training will reuse an existing compilation if no changes were made that force a recompile, and will start from the newest checkpoint file if any. Compiles may be done while another job is using the wafer.</p> <p>See also the current Cerebras quickstart documentation, that uses a clone of Cerebras's abbreviated public \"reference implementations\" github repo rather than the full modelzoo. https://docs.cerebras.net/en/latest/getting-started/cs-tf-quickstart.html https://github.com/Cerebras/cerebras_reference_implementations/</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#running-a-training-job-on-the-wafer-in-weight-streaming-mode","title":"Running a training job on the wafer in weight streaming mode","text":"<p>No CS2-nodes are currently configured for weight streaming mode. This section is currently a placeholder.</p> <p>If not already done, copy the modelzoo tree:</p> <p><pre><code>cd ~/\nmkdir ~/R1.5/\ncp -r /software/cerebras/model_zoo/modelzoo ~/R1.5/modelzoo\n</code></pre> then change to the TensorFlow GPT2 directory: <pre><code>cd ~/R1.5/modelzoo/transformers/tf/gpt2\n</code></pre> then edit the two instances of data_dir in configs/params_gpt2_small_ws.yaml (or in a copy of that file) as follows: <pre><code>&lt;     data_dir: \"./input/pile_pretraining_gpt/train_msl2048/\"\n---\n&gt;     data_dir: \"/software/cerebras/dataset/transformers/owt/openwebtext/owt_tfrecords_gpt2_msl2048/train/\"\n&lt;     data_dir: \"./input/pile_pretraining_gpt/val_msl2048/\"\n---\n&gt;     data_dir: \"/software/cerebras/dataset/transformers/owt/openwebtext/owt_tfrecords_gpt2_msl2048/val/\"\n</code></pre> then <pre><code>csrun_wse --cyclic --total-nodes=4 --single-task-nodes=2 python-ws run.py  -p configs/params_gpt2_small.yaml  -m train --model_dir gpt2_small_owt_2048 --cs_ip $CS_IP\n</code></pre></p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#running-a-training-job-on-the-cpu","title":"Running a training job on the CPU","text":"<p>The examples in the modelzoo will run in CPU mode, either using the csrun_cpu script, or in a singularity shell as shown below.</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#using-csrun_cpu","title":"Using csrun_cpu","text":"<p>To separately compile and train,</p> <pre><code># delete any existing compile artifacts and checkpoints\nrm -r model_dir\ncsrun_cpu python run.py --mode train --compile_only\ncsrun_cpu python run.py --mode train --max_steps 400\n</code></pre> <p>Note: If no cs_ip is specified, a training run will be in cpu mode. </p> <p>Change the max steps for the training run command line to something smaller than the default so that the training completes in a reasonable amount of time. (CPU mode is &gt;2 orders of magnitude slower for many examples.)</p>"},{"location":"cerebras/Steps-to-run-a-model-or-program/#using-a-singularity-shell","title":"Using a singularity shell","text":"<p>This illustrates how to create a singularity container. The <code>-B /opt:/opt</code> is an illustrative example of how to bind a directory to a singularity container. (The singularity containers by default bind both one's home directory and /tmp, read/write.)</p> <p><pre><code>cd ~/R1.5/modelzoo/fc_mnist/tf\nsingularity shell -B /opt:/opt /software/cerebras/cs2-02/container/cbcore_latest.sif\n</code></pre> or, on cs2-01, <pre><code>cd ~/R1.5/modelzoo/fc_mnist/tf\nsingularity shell -B /opt:/opt /software/cerebras/cs2-01/container/cbcore_latest.sif\n</code></pre></p> <p>At the shell prompt for the container,</p> <pre><code>#rm -r model_dir\n# compile and train on the CPUs\npython run.py --mode train --max_steps 1000\npython run.py --mode eval --eval_steps 1000\n# validate_only is the first portion of a compile\npython run.py --mode train --validate_only\n# remove the existing compile and training artifacts\nrm -r model_dir\n# compile_only does a compile but no training\npython run.py --mode train --compile_only\n</code></pre> <p>Type <code>exit</code> at the shell prompt to exit the container.</p>"},{"location":"cerebras/System-Overview/","title":"System Overview","text":"<p>The Cerebras CS-2 is a wafer-scale deep learning accelerator comprising 850,000 processing cores, each providing 48KB of dedicated SRAM memory for an on-chip total of 40GB and interconnected to optimize bandwidth and latency. Its software platform integrates popular machine learning frameworks such as TensorFlow and PyTorch.</p> <p>For an overview of the Cerebras CS-2 system, see this whitepaper: Cerebras Systems: Achieving Industry Best AI Performance Through A Systems Approach.</p> <p>The public Cerebras documentation is available here.</p> <p>A typical CS-2 cluster is as shown in the figure. On the first Argonne CS-2 cluster(cs2-01), the eight worker nodes and the chief node each have a AMD EPYC 7702P 64-Core Processor totalling 64 cores and 128 GB memory. On the second Argonne CS-2 cluster(cs2-02), the seven worker nodes and the chief node each have Intel(R) Xeon(R) Gold 6248 CPU processors totaling 80 cores and 200GB memory. The <code>/home</code>, <code>/projects</code> and <code>/software</code> trees are shared across both CS2 clusters, and all ALCF AI testbed platforms.</p> <p> (Figure from https://docs.cerebras.net/en/latest/getting-started/checklist-before-you-start.html)</p> <p>As indicated in the figures, a CS system is responsible only for running and accelerating the actual training and predictions with the model.</p> <p>All the supporting tasks such as compiling the model, preprocessing the input data, running the input function, streaming the data, and managing the training loop, are executed in a Cerebras CPU cluster by the Cerebras software running on these nodes.</p> <p></p> <p>(Figure from https://docs.cerebras.net/en/latest/cerebras-basics/how-cerebras-works.html)</p>"},{"location":"cerebras/Tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>See ALCF's Jupyter Instructions, and Tunneling and forwarding ports in the SambaNova documentation.</p>"},{"location":"cerebras/Using-virtual-environments-to-customize-environment/","title":"Customizing Environments","text":""},{"location":"cerebras/Using-virtual-environments-to-customize-environment/#using-a-virtual-python-environment-to-customize-an-environment","title":"Using a virtual python environment to customize an environment","text":"<p>It is considered good practice to create and use a python virtual environment for python projects with dependencies not satisfied by the base environment. This prevents dependency conflicts between projects. Cerebras does not support conda, but python virtualenvs can be used instead.</p> <pre><code>python3 -m venv ~/testp3env\nsource ~/testp3env/bin/activate\npip install --upgrade pip\n# sample pip install\npip install wget\n# to exit the virtual env:\ndeactivate\n</code></pre> <p>To use this virtual env, e.g. in a script started with csrun_wse or csrun_cpu, or in a singularity shell:</p> <pre><code>source testp3env/bin/activate\n</code></pre>"},{"location":"cerebras/Using-virtual-environments-to-customize-environment/#customizing-the-cerebras-singularity-container","title":"Customizing the Cerebras singularity container","text":"<p>See this section of the Cerebras documentation: Adding Custom Packages To cbcore Container</p>"},{"location":"common/pub-policy/","title":"Publication Policy","text":"<p>To publish technical reports and research papers using the ALCF AI testbeds, please provide us with a draft of your paper for review prior to submission by emailing a copy to support@alcf.anl.gov. We will work closely with our AI testbed vendors to review and provide feedback in a timely manner. We strongly recommend that you engage us and the vendors <code>early</code> and <code>often</code> in this process to help us facilitate your research objectives.</p> <p>You can find ALCF Acknowledgement policy at: https://www.alcf.anl.gov/support-center/facility-policies/alcf-acknowledgement-policy</p>"},{"location":"common/storage/","title":"Storage","text":""},{"location":"common/storage/#home-file-system-space","title":"Home File System Space","text":"<p>Users have a shared home filesystem <code>/home</code> shared across the ALCF AI testbed systems, including the login and compute nodes. Default user quota is <code>1 TB</code> storage and <code>1,000,000 files</code>. This space is backed up. </p>"},{"location":"common/storage/#project-file-system-space","title":"Project File System Space","text":"<p>The team project/campaign file system <code>/projects</code> is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account.  Default group storage quota is <code>2 TB</code> and <code>2,000,000 files</code>. Please note that this space isn't backed up.  Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"common/storage/#data-transfer","title":"Data Transfer","text":"<p>Users can transfer data to and from the AI testbed using <code>Globus</code> or tools such as <code>scp</code> or <code>rsync</code>.</p>"},{"location":"common/storage/#using-globus","title":"Using Globus","text":"<p>We have a Globus endpoint each to move data to and from the <code>/projects</code> and <code>/home</code> filesystem respectively.</p> <ul> <li>Use <code>alcf#ai_testbed_projects</code> for the <code>/projects</code> file system</li> <li>Use <code>alcf#ai_testbed_home</code> for the <code>/home</code> files system </li> </ul> <p>Relevant information regarding using globus can be found here</p>"},{"location":"common/storage/#alcf-facility-policies","title":"ALCF Facility Policies","text":"<p>ALCF Facility Policies is available here </p> <p>Please Note: The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p>"},{"location":"habana/Getting-Started/","title":"Getting Started","text":""},{"location":"habana/Getting-Started/#on-boarding","title":"On-Boarding","text":"<p>See Get Started to request an acccount and additional information.</p>"},{"location":"habana/Getting-Started/#setup","title":"Setup","text":""},{"location":"habana/Getting-Started/#system-view","title":"System View","text":"<p>Connection to a Sambanova node is a two step process. First step is to ssh to a \"login node\". This step requires a MFA passcode for authentication - a 8 digit passcode generated by an app on your mobile device (e.g. mobilePASS+). The second step is to login to a sambanova node from the login node.  In the examples below, replace ALCFUserID with your ALCF user id. </p>"},{"location":"habana/Getting-Started/#login-to-login-node","title":"Login to Login Node","text":"<p>Login to the SambaNova login node from your local machine using the below command. This uses the MobilPass+ token generated everytime you login to the system. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nALCFUserID@sambanova.alcf.anl.govs password: &lt; MobilPass+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"habana/Getting-Started/#login-to-sambanova-node","title":"Login to SambaNova Node","text":"<p>Once you are on the login node, the sambanova system can be accessed using the alias \u201csm-01\u201d that resolves to hostname sm-01.ai.alcf.anl.gov. </p> <pre><code>ssh sm-01\n</code></pre>"},{"location":"habana/Getting-Started/#sdk-setup","title":"SDK setup","text":"<p>The SambaNova system has a bash shell script to setup the required software environment. This sets up the SambaFlow software stack, the associated environmental variables and activates a pre-configured virtual environment.</p> <p>Use</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>The contents of the sn_env.sh script is shown below for convenience.</p> <pre><code>alias snpath='export PATH=$PATH:/opt/sambaflow/bin' # This is the path to SambaFlow which is the software stack that is running on SambaNova systems. This stack includes the Runtime, the compilers, and the SambaFlow Python SDK which is used to create and run models.\n\nalias snthreads='export OMP_NUM_THREADS=1' # The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions. The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels.For the SambaNova system it is usually set to 1.\n\nalias snvenv='source /opt/sambaflow/venv/bin/activate' # This activates the pre-configured virtual environment that consists of sambaflow and other built-in libraries.\n</code></pre> <p>NOTE:\u00a0 SambaNova operations will fail unless the SambaNova venv is set up.</p> <p>You may deactivate the environment if finished.</p> <pre><code>deactivate\n</code></pre>"},{"location":"sambanova/Documentation/","title":"Documentation","text":"<p>The SambaNova documentation is housed at <code>/software/sambanova/docs/latest/</code> accessible via login node.</p> <pre><code>getting-started.pdf             # Getting Started with SambaFlow\naccuracy-debugging-tools.pdf    # Introduction to the model accuracy debugging tools.\ncompiler-options.pdf            # Provides advanced compiler options for the compile command.\nconversion-to-sambaflow.pdf     # Converting existing models to SambaFlow\nintermediate-tutorial.pdf       # SambaFlow intermediate model\nintro-tutorial-pytorch.pdf      # A peek into the code of the above example program.\nrelease-notes.pdf               # Provide new feature and bug fixes updates for each release version.\nrun-examples-language.pdf       # Run BERT on RDU\nrun-examples-pytorch.pdf        # Run Power PCA and micro models like GEMM on RDU\nrun-examples-vision.pdf         # Run UNET on RDU.\nusing-layernorm.pdf             # Example to use LayerNorm instead of BatchNorm\nusing-venvs.pdf                 # Python Virtual Environment.\n\nlatest\\\n    accuracy-debugging-tools.pdf\n    compiler-options.pdf\n    conversion-to-sambaflow.pdf\n    getting-started.pdf\n    intermediate-tutorial.pdf\n    intro-tutorial-pytorch.pdf\n    release-notes.pdf\n    run-examples-language.pdf\n    run-examples-pytorch.pdf\n    run-examples-vision.pdf\n    using-layernorm.pdf\n    using-venvs.pdf\n</code></pre> <p>The documentation can be viewed on your local system by copying the files from the login node.</p> <pre><code>cd &lt;your docs destination&gt;\nscp -r ALCFUserID@sambanova.alcf.anl.gov:/software/sambanova/docs/latest/* .\n</code></pre> <p>View the PDFs in your favorite viewer or web browser on your local machine.</p>"},{"location":"sambanova/Example-Multi-Node-Programs/","title":"Example Multi-Node Programs","text":"<p>SambaNova provides examples of some well-known AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on both SambaNova compute nodes. Make a copy of this to your home directory:</p> <p>Copy starters to your personal directory structure if you have not already done so.</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre>"},{"location":"sambanova/Example-Multi-Node-Programs/#unet","title":"UNet","text":""},{"location":"sambanova/Example-Multi-Node-Programs/#set-up","title":"Set-up","text":"<p>Copy files and change directory if you have not already done so.</p> <pre><code>cp -r /opt/sambaflow/apps/image ~/apps/image\ncd ~/apps/image\ncp /software/sambanova/apps/image/pytorch/unet/*.sh .\n</code></pre> <p>You just copied two bash scripts.  They are:</p> <ol> <li> <p>unet_all.sh</p> <ul> <li>Compiles UNet and then submits a batch job to run the model.</li> </ul> </li> <li> <p>unet_batch.sh</p> <ul> <li>Runs Unet.</li> </ul> </li> </ol>"},{"location":"sambanova/Example-Multi-Node-Programs/#unet-all","title":"Unet All","text":"<p>Here is a breakdown of unet_all.sh.</p> <p>The argument -x is used to specify that each executed line is to be displayed.</p> <p>The second line is to stop on error.</p> <p>Lastly, set total time, SECONDS, to zero.</p> <pre><code>#! /bin/bash -x\nset -e\n#\n# Usage: ./unet_all.sh 256 256\n#\nSECONDS=0\n</code></pre> <p>Set variables.</p> <pre><code># IMage size.\nIM=${1}\n# Batch Size\nBS=${2}\nNUM_WORKERS=1\nexport OMP_NUM_THREADS=16\n</code></pre> <p>Activate the virtual environment.  And, establish the UNet directory.</p> <pre><code>source /opt/sambaflow/venv/bin/activate\nUNET=$(pwd)/unet\n</code></pre> <p>Display model name and time.</p> <pre><code>echo \"Model: UNET\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\n\necho \"COMPILE\"\n</code></pre> <p>This section will compile the model for multiple RDUs if it does not exist.</p> <p>A log file will be created at compile_${BS}_${IM}_NN.log.</p> <pre><code># Compile for parallel RDUs\nif [ ! -e out/unet_train_${BS}_${IM}_NN/unet_train_${BS}_${IM}_NN.pef ] ; then\n  python ${UNET}/unet.py compile -b ${BS} --in-channels=3 --in-width=${IM} --in-height=${IM} --enable-conv-tiling --mac-v2 --compiler-configs-file ${UNET}/jsons/compiler_configs/unet_compiler_configs_no_inst.json --pef-name=\"unet_train_${BS}_${IM}_NN\"  --data-parallel -ws 2 &gt; compile_${BS}_${IM}_NN.log 2&gt;&amp;1\nfi\n</code></pre> <p>Here, a batch job is submitted for the multi-node run.</p> <p>Sbatch argument definitions:</p> <ul> <li> <p>--gres=rdu:1</p> <p>This indicates that the model fits on a single RDU.</p> </li> <li> <p>--tasks-per-node 8</p> <p>All eight RDUs per node are to be used.  Valid options are 1 through 8.</p> </li> <li> <p>--nodes 2</p> <p>The number of nodes to use.  Currently there are two nodes.</p> </li> <li> <p>--nodelist sm-02,sm-01</p> <p>The node names to use.</p> </li> <li> <p>--cpus-per-task=16</p> <p>CPUs per model.</p> </li> <li> <p>unet_batch.sh</p> <p>The bash script to be batched.</p> </li> </ul> <p>Unet_batch.sh argument definitions:</p> <ul> <li> <p>NN</p> <p>Number of nodes.</p> </li> </ul> <pre><code># Run Multi-Node, Data Parallel\nNN=2\necho \"RUN\"\necho \"NN=${NN}\"\nsbatch --gres=rdu:1 --tasks-per-node 8  --nodes 2 --nodelist sm-02,sm-01 --cpus-per-task=16 ./unet_batch.sh ${NN} ${NUM_WORKERS}\necho \"Duration: \" $SECONDS\n</code></pre>"},{"location":"sambanova/Example-Multi-Node-Programs/#unet-batch","title":"Unet Batch","text":"<p>Here is a description of unet_batch.sh.  This script is automatically run by unet_all.sh.</p> <p>This block is the same as above.</p> <pre><code>#! /bin/bash -x\nset -e\n#\n# Usage: ./unet_batch.sh 2 1\n#\nSECONDS=0\n</code></pre> <p>Establish variables.</p> <pre><code># Batch Size\nBS=256\n\n# IMage size\nIM=256\nNN=${1}\nNUM_WORKERS=${2}\nexport OMP_NUM_THREADS=16\nDATADIR=/software/sambanova/dataset/kaggle_3m\nUNET=$(pwd)/unet\nexport SAMBA_CCL_USE_PCIE_TRANSPORT=0\n</code></pre> <p>Activate virtual environment.</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Display an informative banner.</p> <pre><code>echo \"Model: UNET_TRAIN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\n</code></pre> <p>Run Unet</p> <pre><code>srun --mpi=pmi2 python ${UNET}/unet_hook.py  run --do-train --in-channels=3 --in-width=${IM} --in-height=${IM} --init-features 32 --batch-size=${BS} --epochs 2   --data-dir ${DATADIR} --log-dir log_dir_unet_${NN}_train_kaggle --pef=$(pwd)/out/unet_train_${BS}_${IM}_NN/unet_train_${BS}_${IM}_NN.pef --data-parallel --reduce-on-rdu --num-workers=${NUM_WORKERS}\n</code></pre> <p>Display total execution time.</p> <pre><code>echo \"Duration: \" $SECONDS\n</code></pre>"},{"location":"sambanova/Example-Multi-Node-Programs/#compile-and-run","title":"Compile and Run","text":"<p>Change directory:</p> <pre><code>cd ~/apps/image/\n</code></pre> <p>Compile and run UNet:</p> <pre><code>./unet_all.sh 256 256\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n</code></pre>"},{"location":"sambanova/Example-Programs/","title":"Example Programs","text":"<p>SambaNova provides examples of some well-known AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on both SambaNova compute nodes. Make a copy of this to your home directory:</p> <p>Copy starters to your personal directory structure:</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre>"},{"location":"sambanova/Example-Programs/#lenet","title":"LeNet","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/lenet\n</code></pre>"},{"location":"sambanova/Example-Programs/#common-arguments","title":"Common Arguments","text":"<p>Below are some of the common arguments used across most of the models in the example code.</p> Argument Default Help -b 1 Batch size for training -n, 100 Number of iterations to run --num-iterations the pef for -e, 1 Number epochs for training --num-epochs --log-path 'check Log path points' --num-workers 0 Number of workers --measure-train- None Measure training performance performance"},{"location":"sambanova/Example-Programs/#lenet-arguments","title":"LeNet Arguments","text":"Argument Default Help --lr 0.01 Learning rate for training --momentum 0.0 Momentum value for training --weight-decay 0.01 Weight decay for training --data-path './data' Data path --data-folder 'mnist_ Folder containing mnist data data' <p>NOTE:\u00a0 If you receive an \\\"HTTP error\\\" message on any of the following commands, run the command again. Such errors (e.g 503) are commonly an intermittent failure to download a dataset.</p> <p>Run these commands:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>To use Slurm sbatch, create submit-lenet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\n\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n# One may also...\nwatch squeue\n</code></pre> <p>The output file will look something like this:</p> <pre><code>[Info][SAMBA][Default] # Placing log files in\npef/lenet/lenet.samba.log\n\n[Info][MAC][Default] # Placing log files in\npef/lenet/lenet.mac.log\n\n[Warning][SAMBA][Default] #\n\n--------------------------------------------------\n\nUsing patched version of torch.cat and torch.stack\n\n--------------------------------------------------\n\n[Warning][SAMBA][Default] # The dtype of \"targets\" to\nCrossEntropyLoss is torch.int64, however only int16 is currently\nsupported, implicit conversion will happen\n\n[Warning][MAC][GraphLoweringPass] # lenet__reshape skip\nset_loop_to_air\n\n[Warning][MAC][GraphLoweringPass] # lenet__reshape_bwd skip\nset_loop_to_air\n\n...\n\nEpoch [1/1], Step [59994/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59995/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59996/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59997/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59998/60000], Loss: 0.1712\n\nEpoch [1/1], Step [59999/60000], Loss: 0.1712\n\nEpoch [1/1], Step [60000/60000], Loss: 0.1712\n\nTest Accuracy: 98.06 Loss: 0.0628\n\n2021-6-10 10:52:28 : [INFO][SC][53607]: SambaConnector: PEF File:\npef/lenet/lenet.pef\n\nLog ID initialized to: [ALCFUserID][python][53607] at\n/var/log/sambaflow/runtime/sn.log\n</code></pre>"},{"location":"sambanova/Example-Programs/#mnist-feed-forward-network","title":"MNIST - Feed Forward Network","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/ffn_mnist/\n</code></pre> <p>Commands to run MNIST example:</p> <pre><code>srun python ffn_mnist.py compile --pef-name=\"ffn_mnist\" --output-folder=\"pef\"\nsrun python ffn_mnist.py run --pef=\"pef/ffn_mnist/ffn_mnist.pef\" --data-path mnist_data\n</code></pre> <p>To run the same using Slurm sbatch, create and run the submit-ffn_mnist-job.sh with the following contents.</p> <pre><code>#!/bin/sh\npython ffn_mnist.py compile --pef-name=\"ffn_mnist\" --output-folder=\"pef\"\npython ffn_mnist.py run --pef=\"pef/ffn_mnist/ffn_mnist.pef\" --data-path mnist_data\n</code></pre> <pre><code>sbatch --output=pef/ffn_mnist/output.log submit-ffn_mnist-job.sh\n</code></pre>"},{"location":"sambanova/Example-Programs/#logistic-regression","title":"Logistic Regression","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/logreg\n</code></pre>"},{"location":"sambanova/Example-Programs/#logistic-regression-arguments","title":"Logistic Regression Arguments","text":"<p>This is not an exhaustive list of arguments.</p> <p>Arguments</p> Argument Default Help Step --lr 0.001 Learning rate for training Compile --momentum 0.0 Momentum value for training Compile --weight-decay 1e-4 Weight decay for training Compile --num-features 784 Number features for training Compile --num-classes 10 Number classes for training Compile --weight-norm na Enable weight normalization Compile <p>Run these commands:</p> <pre><code>srun python logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\nsrun python logreg.py test --pef=\"pef/logreg/logreg.pef\"\nsrun python logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>To use Slurm, create submit-logreg-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\npython logreg.py test --pef=\"pef/logreg/logreg.pef\"\npython logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>Then</p> <pre><code>sbatch --output=pef/logreg/output.log submit-logreg-job.sh\n</code></pre> <p>The output, pef/logreg/output.log, will look something like this:</p> <pre><code>[Info][SAMBA][Default] # Placing log files in\npef/logreg/logreg.samba.log\n[Info][MAC][Default] # Placing log files in\npef/logreg/logreg.mac.log\n[Warning][SAMBA][Default] #\n--------------------------------------------------\nUsing patched version of torch.cat and torch.stack\n--------------------------------------------------\n\n[Warning][SAMBA][Default] # The dtype of \"targets\" to\nCrossEntropyLoss is torch.int64, however only int16 is currently\nsupported, implicit conversion will happen\n[Warning][MAC][MemoryOpTransformPass] # Backward graph is trimmed\naccording to requires_grad to save computation.\n[Warning][MAC][WeightShareNodeMergePass] # Backward graph is\ntrimmed according to requires_grad to save computation.\n[Warning][MAC][ReduceCatFaninPass] # Backward graph is trimmed\naccording to requires_grad to save computation.\n[info ] [PLASMA] Launching plasma compilation! See log file:\n/home/ALCFUserID/apps/starters/pytorch/pef/logreg//logreg.plasma_compile.log\n...\n\n[Warning][SAMBA][Default] # The dtype of \"targets\" to\nCrossEntropyLoss is torch.int64, however only int16 is currently\nsupported, implicit conversion will happen\nEpoch [1/1], Step [10000/60000], Loss: 0.4763\nEpoch [1/1], Step [20000/60000], Loss: 0.4185\nEpoch [1/1], Step [30000/60000], Loss: 0.3888\nEpoch [1/1], Step [40000/60000], Loss: 0.3721\nEpoch [1/1], Step [50000/60000], Loss: 0.3590\nEpoch [1/1], Step [60000/60000], Loss: 0.3524\nTest Accuracy: 90.07 Loss: 0.3361\n2021-6-11 8:38:49 : [INFO][SC][99185]: SambaConnector: PEF File:\npef/logreg/logreg.pef\nLog ID initialized to: [ALCFUserID][python][99185] at\n/var/log/sambaflow/runtime/sn.log\n</code></pre>"},{"location":"sambanova/Example-Programs/#unet","title":"UNet","text":"<p>Change directory and copy files.</p> <pre><code>cp -r /opt/sambaflow/apps/image ~/apps/image\ncd ~/apps/image/unet\ncp /software/sambanova/apps/image/pytorch/unet/*.sh .\n</code></pre> <p>Export the path to the dataset which is required for the training.</p> <pre><code>export OUTDIR=~/apps/image/unet\nexport DATADIR=/software/sambanova/dataset/kaggle_3m\n</code></pre> <p>Run these commands for training (compile + train):</p> <pre><code>sbatch unet_compile_run_inf_rl.sh compile 32 1  # Takes over 15 minutes.\nsbatch unet_compile_run_inf_rl.sh test 32 1     # Very fast.\nsbatch unet_compile_run_inf_rl.sh run 32 1      #\n</code></pre> <p>The output files are named slurm-\\&lt;batch ID&gt;.out.</p> <p>Using SLURM:  To use Slurm, create submit-unet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\nexport OUTDIR=~/apps/image/unet\nexport DATADIR=/software/sambanova/dataset/kaggle_3m\n./unet_compile_run_inf_rl.sh compile 32 1\n./unet_compile_run_inf_rl.sh test 32 1\n./unet_compile_run_inf_rl.sh run 32 1\n</code></pre> <p>Then</p> <pre><code>sbatch submit-unet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n</code></pre>"},{"location":"sambanova/Job-Queuing-and-Submission/","title":"Job Queueing and Submission","text":""},{"location":"sambanova/Job-Queuing-and-Submission/#introduction","title":"Introduction","text":"<p>SambaNova uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>NOTE: Run the python scripts using srun or sbatch, to ensure that concurrent jobs do not interfere with each other.</p> <p>NOTE: There is just one scheduler for both sm-01 and sm-02.</p>"},{"location":"sambanova/Job-Queuing-and-Submission/#srun","title":"Srun","text":"<p>The Slurm command <code>srun</code> can be used to run individual python scripts in parallel with other scripts on a cluster managed by Slurm. Examples of <code>srun</code> usage are shown below.</p> <p>Slurm will assign a nodelist/host to run a job if a host is not specified.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>You may specify which node/host on which to run a job.</p> <p>Reasons to specify a node list:</p> <ul> <li>One wants to test a specific node to verify function of the HW and SW  (daily smoke tests do this)</li> <li>The nodes are at different software levels and one wants to use a node that has the needed software level for one's application.</li> </ul> <p>Example:</p> <pre><code>srun --nodelist=sm-02 python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre>"},{"location":"sambanova/Job-Queuing-and-Submission/#sbatch","title":"Sbatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-lenet-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\n\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below.</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>In case of the need to use multiple RDUs (2 in the example shown below), the <code>sbatch</code> command would be altered as:</p> <pre><code>sbatch --gres=rdu:2 &lt;your_script.sh&gt;\n</code></pre>"},{"location":"sambanova/Job-Queuing-and-Submission/#squeue","title":"Squeue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>squeue\n</code></pre>"},{"location":"sambanova/Job-Queuing-and-Submission/#sinfo","title":"Sinfo","text":"<p>Sinfo is used to view partition and node information for a system running Slurm.</p> <p>Here is a suggested command:</p> <pre><code>sinfo -O AllocNodes, GresUsed, Gres, NodeList\n</code></pre> <p>For more information, see sinfo.</p>"},{"location":"sambanova/Job-Queuing-and-Submission/#scancel","title":"Scancel","text":"<p>Scancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"sambanova/Logging-into-a-SambaNova-Node/","title":"Getting Started","text":""},{"location":"sambanova/Logging-into-a-SambaNova-Node/#on-boarding","title":"On-Boarding","text":"<p>See Get Started to request an account and additional information.</p>"},{"location":"sambanova/Logging-into-a-SambaNova-Node/#setup","title":"Setup","text":""},{"location":"sambanova/Logging-into-a-SambaNova-Node/#system-view","title":"System View","text":"<p>Connection to a SambaNova node is a two-step process. The first step is to ssh to the login node. This step requires an MFA passcode for authentication - an eight-digit passcode generated by an app on your mobile device, e.g., mobilePASS+. The second step is to log in to a SambaNova node from the login node.</p> <p></p>"},{"location":"sambanova/Logging-into-a-SambaNova-Node/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Login to the SambaNova login node from your local machine using the below command. This uses the MobilPass+ token generated every time you log in to the system. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley.</p> <p>In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nPassword: &lt; MobilPass+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"sambanova/Logging-into-a-SambaNova-Node/#log-in-to-a-sambanova-node","title":"Log in to a SambaNova Node","text":"<p>Once you are on the login node, the SambaNova system can be accessed using the alias sm-01 or sm-02.</p> <pre><code>ssh sm-01\n# or\nssh sm-02\n</code></pre>"},{"location":"sambanova/Logging-into-a-SambaNova-Node/#sdk-setup","title":"SDK setup","text":"<p>The SambaNova system has a bash shell script to set up the required software environment. This sets up the SambaFlow software stack, and the associated environmental variables and starts a pre-configured virtual environment.</p> <p>Use</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>The contents of the sn_env.sh script is shown below for convenience.</p> <pre><code>alias snpath='export PATH=$PATH:/opt/sambaflow/bin' # This is the path to SambaFlow which is the software stack running on SambaNova systems. This stack includes the Runtime, the compilers, and the SambaFlow Python SDK which is used to create and run models.\n\nalias snthreads='export OMP_NUM_THREADS=16' # The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions. The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels. For the SambaNova system, it is usually set to 1.\n\nalias snvenv='source /opt/sambaflow/venv/bin/activate' # This starts the pre-configured virtual environment that consists of sambaflow and other built-in libraries.\n</code></pre> <p>NOTE:\u00a0 SambaNova operations will fail unless the SambaNova venv is set up.</p> <p>You may deactivate the environment if finished.</p> <pre><code>deactivate\n</code></pre>"},{"location":"sambanova/Miscellaneous/","title":"Miscellaneous","text":""},{"location":"sambanova/Miscellaneous/#sdk-version","title":"SDK Version","text":"<p>To find the SDK version, run the following commands</p> <pre><code>(venv) ALCFUserID@sm-01:~$ python\nPython 3.7.6 (default, Feb 18 2020, 21:28:31) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import sambaflow\n&gt;&gt;&gt; sambaflow.__version__\n'1.11.5'\n&gt;&gt;&gt; \n</code></pre>"},{"location":"sambanova/Miscellaneous/#omp_num_threads","title":"OMP_NUM_THREADS","text":"<p>The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions.</p> <p>The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels.</p> <p>For the SambaNova system it, is usually set to one.</p> <pre><code>export OMP_NUM_THREADS=1\n</code></pre>"},{"location":"sambanova/Miscellaneous/#where-is-the-model","title":"Where is the Model?","text":"<p>Two copies of the model are maintained.\u00a0 One in CPU memory and one in RDU memory. They do not interfere with each other unless you explicitly sync the model/parameter in between using:</p> <pre><code>SambaTensor.rdu() # Moves the CPU model to the RDU\nSambaTensor.cpu() # Moves the RDU model to the CPU\n</code></pre> <p>In order to run the model on the CPU, you can simply use the PyTorch model as if there is no RDU. In order to run the model on RDU, you would need to use session.run().</p>"},{"location":"sambanova/Miscellaneous/#useful-commands","title":"Useful Commands","text":""},{"location":"sambanova/Miscellaneous/#sn-configuration","title":"SN Configuration","text":"<pre><code>snconfig\n</code></pre> <p>The snconfig utility shows the static configuration of the system. The configuration on sm-01 for the first RDU is as follows:</p> <pre><code>Platform Name: DataScale SN10-8\nNode Name: NODE\nNumber of XRDUS: 4\nXRDU Name: XRDU_0\nNumber of RDUS: 2\nRDU name: RDU_0\nNumber of TILES: 4\nTILE Name: TILE_0\nSerial Number : N/A\n...\nNumber of PCIES: 4\nPCIE Name: PCIE_0\nBandwidth : 32 GB/s\nSpeed : 16 GT/s\nWidth : 16\nSerial Number : N/A\n...\nNumber of DDRCHs: 6\nDDR CH Name: DDRCH_0\nNumber of DIMMS: 2\nDIMM Name: DIMM_C0\nSize : 64.0 GB\nDIMM Name: DIMM_C1\nSize : 0.0 GB\nSerial Number : N/A\nCurrent utilization can be seen with sntilestat. In this example, only\nfour tiles in one RDU are in use.\nTILE %idle %exec %pload %aload %chkpt %quiesce PID USER COMMAND\n/XRDU_0/RDU_0/TILE_0 80.4 7.0 10.4 2.2 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_0/TILE_1 80.5 6.9 11.3 1.3 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_0/TILE_2 82.1 4.7 11.4 1.8 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_0/TILE_3 80.1 6.3 11.7 1.9 0.0 0.0 49880 arnoldw python\nres_ffn_mnist.py run --pef=pef/res_ffn_mnist/res_ffn_mnist.pef\n--num-epochs 100\n/XRDU_0/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_0/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_0/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_0/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_0/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_1/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_0/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_2/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_0/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_0 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_1 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_2 100.0 0.0 0.0 0.0 0.0 0.0\n/XRDU_3/RDU_1/TILE_3 100.0 0.0 0.0 0.0 0.0 0.0\n</code></pre>"},{"location":"sambanova/Miscellaneous/#sambanova-daemon-service","title":"SambaNova Daemon Service","text":"<p>The following command checks if the SambaNova daemon service is running.</p> <pre><code>systemctl status snd\n</code></pre> <p>The output should look something like this:</p> <pre><code>* snd.service - SN Devices Service\n   Loaded: loaded (/usr/lib/systemd/system/snd.service; enabled; vendor preset: enabled)\n   Active: active (running) since Fri 2022-02-18 11:45:15 CST; 1 months 25 days ago\n Main PID: 3550 (snd)\n    Tasks: 10 (limit: 19660)\n   CGroup: /system.slice/snd.service\n           `-3550 /opt/sambaflow/bin/snd\n\nWarning: Journal has been rotated since the unit was started. Log output is incomplete or unavailable.\n</code></pre>"},{"location":"sambanova/Miscellaneous/#tile-status","title":"Tile status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre> <p>The output shown below is when the system is completely idle.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n</code></pre>"},{"location":"sambanova/Miscellaneous/#finding-hung-tiles","title":"Finding Hung Tiles","text":"<pre><code>snconfig show Node dynamic | grep perfect\n</code></pre>"},{"location":"sambanova/Miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"sambanova/Performance-Tools/","title":"Performance Tools","text":""},{"location":"sambanova/Performance-Tools/#tile-status","title":"Tile Status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre>"},{"location":"sambanova/Performance-Tools/#measure-tflops","title":"Measure TFLOPs","text":"<p>This is an example for measuring TFLOPs for Conv2D forward pass.</p> <pre><code>elif args.command == 'run':\n    samba.session.run(inputs, section_types=['fwd'])\n    #samba.session.run(inputs, section_types=['bckwd'])\n    n_iters = 100\n    forward_pass_time = []\n    print(\"run starts\")\n    start_time_forward = time.time()\n    for loop in range(n_iters):\n        samba.session.run(inputs, section_types=['fwd'])\n        #samba.session.run(inputs, section_types=['bckwd'])\n        #samba.session.run(inputs, section_types=['fwd', 'bckwd'])\n    end_time_forward = time.time()\n    forward_pass_time.append(end_time_forward - start_time_forward)\n    print(\"run ends\")\n\n    w_0 = (args.w + 2*args.pad_w - args.s)/args.wstride + 1\n    h_0 = (args.h + 2*args.pad_h - args.r)/args.hstride + 1\n    tflops = 2 * (w_0*h_0) * args.s * args.r * args.c * args.k * args.n\n    tflops_forw = tflops/(sum(forward_pass_time)/n_iters/5)/(10**12) #tflops\n    print(tflops)\n    print(sum(forward_pass_time))\n    print(\"tflops: %f\"%tflops_forw)\n    print(\"SN,Training,%s,Conv2d_fwd,%d,100,1,%d,%d,%d,%d,%d,%d,%d,0.0,%f,None,%f,%f,%f\" % (\"dtype\", args.n, args.w, args.h, args.c, args.k, args.s, args.pad_w, args.wstride, (sum(forward_pass_time)/n_iters)/args.n, args.n/(sum(forward_pass_time)/n_iters), tflops_forw, (sum(forward_pass_time)/n_iters)/args.n))\n</code></pre>"},{"location":"sambanova/Readme_Rick_02/","title":"Notes","text":"<pre><code>source /software/sambanova/envs/sn_env.sh\nsource ~/.bashrc\ncd ~/tmp\ncp -rf /home/rweisner/tmp/unet .\ncd ~/tmp/unet\nexport OUTDIR=~/apps/image/unet\nexport DATADIR=/software/sambanova/dataset/kaggle_3m\nsbatch --gres=rdu:1 --tasks-per-node 4  --nodes 2 --nodelist sm-02,sm-01 --cpus-per-task=16 ./unet_batch.sh ${NP} ${NUM_WORKERS}\n./unet_compile_run_all.sh compile 256 256\nll\n</code></pre> <pre><code>On sm-01 or sm-02\n/home/rweisner/tmp/gpt for mpirun\n/home/rweisner/tmp/unet\nunet_compile_run_all.sh for slurm\n./unet_compile_run_all.sh compile 256 256\n</code></pre>"},{"location":"sambanova/SambaTune-User-Guide/","title":"SambaTune","text":""},{"location":"sambanova/SambaTune-User-Guide/#notes","title":"Notes","text":"<pre><code>cd /home/rweisner/tmp/uno_test\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@homes.cels.anl.gov\nssh sm-02\nMobilePass+ password\nOn sm-02\nsource /opt/sambaflow/venv/bin/activate\nexport PATH=/opt/sambaflow/bin:$PATH\nsambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\nsambatune_ui --directory /home/wilsonb/tmp/sambatune_gen --port 8580\n#There will be a username and password displayed that you will use in your browser on your laptop.\nCommand used on laptop for port forward\nssh -XL 8580:127.0.0.1:8580 wilsonb@sm-02.cels.anl.gov\nMobilePass+ password\n# You will be logged into sm-02 but, you do not need to do anything.\naddress used in browser on laptop localhost:8580\n#Use username and password from sambatune_ui.\nUsername\nPassword\n\n#TODOBRW\n/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/2022-09-21T19-21-05.html\n</code></pre>"},{"location":"sambanova/SambaTune-User-Guide/#about-sambatune","title":"About SambaTune","text":"<p>SambaTune is a tool for profiling, debugging, and tuning the performance of applications running on SN hardware.</p> <p>The tool automates the collection of hardware performance counters, metrics aggregation, report generation, and visualization. It also automates benchmarking of the application to compute average throughput over a sufficient number of runs. The tool is designed to aid the user with performance bottleneck analysis and tuning.</p> <p>SambaTune is currently used by SN engineers involved in performance tuning efforts. SambaTune is also planned for release to external customers to aid with performance bottleneck analysis and resolution.</p>"},{"location":"sambanova/SambaTune-User-Guide/#run-sambatune","title":"Run SambaTune","text":"<pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <p>First, enter the virtual environment on sm-01 or sm-02:</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Update path:</p> <pre><code>export PATH=/opt/sambaflow/bin:$PATH\n</code></pre>"},{"location":"sambanova/SambaTune-User-Guide/#usage","title":"Usage","text":"<pre><code>usage: sambatune [-h] [--artifact-root ARTIFACT_ROOT] [--disable-override]\n                 [--compile-only | -m MODES [MODES ...]] [--version]\n                 config\n\npositional arguments:\n  config                YAML file with model, compile, run configuration.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --artifact-root ARTIFACT_ROOT\n                        Custom location to save compile/run artifacts;\n                        defaults to '$DUMP_ROOT/artifact_root' (default: None)\n  --disable-override    Reuse the placement from the baseline compilation\n                        (default: False)\n  --compile-only        Run compilation of PEFs for selected modes only\n                        (default: False)\n  -m MODES [MODES ...], --modes MODES [MODES ...]\n                        Select modes to execute from ['benchmark',\n                        'instrument', 'run'] (default: ['benchmark'])\n  --version             version of sambatune and sambaflow.\n</code></pre>"},{"location":"sambanova/SambaTune-User-Guide/#command-overview","title":"Command Overview","text":"<p>By default, it will run with the benchmarking mode enabled. Use the --modes flag to run modes individually or in any combination. Benchmark-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark\n</code></pre> <p>Instrument-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre> <p>All modes:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre>"},{"location":"sambanova/SambaTune-User-Guide/#command-example","title":"Command Example","text":"<pre><code># From Bill\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder=/home/arnoldw//models_dir/1520847 --mac-v1\n\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=/home/arnoldw//models_dir/1520847/uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code># From Bill --&gt; Bruce\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=./uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code>#TODOBRW  This works.  9/19/22\nsm-01/home/wilsonb/tmp/uno_test/uno_ccle.yaml\napp: /opt/sambaflow/apps/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --multiprocess-pickle --use-pickle-train  --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500 --converted-pickle\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_ccle.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\n# Stand-alone\nexport UNO=.\nexport NS=500\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_${NS}_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS}\n\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\n\n\nRicks run python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\u201cout/uno_16_4_${NS}/uno_16_4_${NS}.pef\u201d --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n</code></pre> <pre><code>#TODOBRW\nsm-01/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_brw_CCLE_1_12.yaml\nexport OMP_NUM_THREADS=16\napp: /home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n\n\n\n./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n</code></pre> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n</code></pre> <p>uno_pickl.sh</p> <pre><code>#! /bin/bash -x\n#set -e\nsource /opt/sambaflow/venv/bin/activate\nSECONDS=0\nNS=${2}\nUNO=/opt/sambaflow/apps/private/anl/\nDS=\"ALL\"\nDS=\"CCLE\"\n\nBS=$((NS*16))\nexport OMP_NUM_THREADS=16\n\necho \"Model: UNO_SPA_TRN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\nif [ \"${1}\" == \"convert\" ] ; then\npython3 ${UNO}/uno/uno_data_loaders_converted.py   --in_dir /var/tmp/raw/ --out_dir /software/sambanova/dataset/${DS}_16_${NS}  --batch-size ${BS} --train_sources ${DS} --file-write-frequency 10\n\n\nelif [ \"${1}\" == \"compile\" ] ; then\n  echo \"COMPILE\"\n  python ${UNO}/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision ${UNO}/samba_uno/human_decisions_spatial.json --pef-name=\"uno_16_4_${NS}\" --mac-v1\n\n\nelif [ \"${1}\" == \"run\" ] ; then\n  echo \"RUN ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  #python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n  python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n  #python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\n\nelif [ \"${1}\" == \"pyinstrument\" ] ; then\n  echo \"RUN ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  #python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n  pyinstrument ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n  #python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\n\nelif [ \"${1}\" == \"no_pickle\" ] ; then\n  echo \"no_pickle ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\necho \"PERF\"\npython uno_full.py measure-performance --measure-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --num-iterations 20 --mac-v1\nfi\n\necho \"Duration: \" $SECONDS\n</code></pre> <pre><code>./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n./uno_pickl.sh pyinstrument 500\npyinstrument --load-prev 2022-09-22T18-31-24 -r html\nstdout is a terminal, so saved profile output to /tmp/tmpeo5ehksn.html\ncp /tmp/tmpeo5ehksn.html .\n</code></pre> <p>On dev terminal</p> <pre><code>scp wilsonb@sambanova.alcf.anl.gov:tmp/uno_test/tmpeo5ehksn.html .\n</code></pre> <p>View in local browser.</p>"},{"location":"sambanova/SambaTune-User-Guide/#running","title":"Running","text":"<p>Create a directory for your work.</p> <pre><code>mkdir ~/sambatune\ncd ~/sambatune\n</code></pre> <p>Create small_vae.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/private/anl/moleculevae.py\n\nmodel-args: -b 128 --in-width 512 --in-height 512\n\ncompile-args: compile --plot --enable-conv-tiling --compiler-configs-file /opt/sambaflow/apps/private/anl/moleculevae/compiler_configs_conv.json --mac-v2 --mac-human-decision /opt/sambaflow/apps/private/anl/moleculevae/symmetric_human_decisions_tiled_v2.json\n\nrun-args: --input-path /var/tmp/dataset/moleculevae/ras1_prot-pops.h5 --out-path ${HOME}/moleculevae_out --model-id 0 --epochs 10\n\nenv:\n     OMP_NUM_THREADS: 16\n     SF_RNT_FSM_POLL_BUSY_WAIT: 1\n     SF_RNT_DMA_POLL_BUSY_WAIT: 1\n     CONVFUNC_DEBUG_RUN: 0\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune small_vae.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>Create linear_net.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/micros/linear_net.py\n\nmodel-args: &gt;\n  -b 1024\n  -mb 64\n  --in-features 8192\n  --out-features 4096\n  --repeat 128\n  --inference\n\ncompile-args: &gt;\n  --n-chips 2\n  --plot\n\nenv:\n  SF_RNT_FSM_POLL_BUSY_WAIT: 1\n  SF_RNT_DMA_POLL_BUSY_WAIT: 1\n  CONVFUNC_DEBUG_RUN\": 0\n</code></pre> <p>NOTE: The following takes 45 minutes to run.</p> <p>Run the following example:</p> <pre><code>sambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\ncd ~/tmp/uno_test\nscreen\nsambatune uno.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>where linear_net.yaml is a user-specified configuration file you created above.</p>"},{"location":"sambanova/SambaTune-User-Guide/#sambatune-ui","title":"SambaTune UI","text":""},{"location":"sambanova/SambaTune-User-Guide/#port-availability","title":"Port Availability","text":"<p>It is recommended that you check if the port you want to use is available. You may check by:</p> <pre><code>ps -elf | grep desired_port\n</code></pre> <p>Example:</p> <pre><code>ps -elf | grep 8576\n</code></pre> <p>Alternatively, you may check for all ports in use by sambatune_ui:</p> <pre><code>ps -elf | grep sambatune_ui\n</code></pre> <p>If you need to free a port that you are finished with, you may use the kill command.</p>"},{"location":"sambanova/SambaTune-User-Guide/#start-sambatune-ui","title":"Start SambaTune UI","text":"<p>If you followed the above directions, your artifact_root will be at ~/sambatune/artifact_root.</p> <p>Start the UI:</p> <p>It will tell you the username and password.</p> <p>NOTE: It is recommended to use a port other than 8576 in case someone else is using it.  Select another port close to 8576.</p> <p>Next</p> <pre><code>sambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8576\n</code></pre> <pre><code>#TODOBRW\nsambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8580\nsambatune_ui --directory /home/wilsonb/tmp/uno_test/artifact_root/sambatune_gen --port 8580\nusername: \"admin\", password: \"4f7cac2c-351e-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"aaf1fc88-35c8-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"bf64e4f8-3831-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"8feca89e-384c-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"355222d6-3a88-11ed-93a3-f7ef9c6e5d46\"\n</code></pre> <p>You will see something like:</p> <pre><code>with the,\n    username: \"admin\", password: \"05c63938-2941-11ed-93a3-f7ef9c6e5d46\"\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Starting gunicorn 20.1.0\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Listening at: http://0.0.0.0:8576 (1344959)\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Using worker: sync\n[2022-08-31 15:24:36 +0000] [1345092] [Info] Booting worker with pid: 1345092\n[2022-08-31 15:24:36 +0000] [1345093] [Info] Booting worker with pid: 1345093\n</code></pre> <p>NOTE: Write down the username and password.</p> <p>NOTE: The password only works with this one instance of sambatune_ui.  If you stop this instance of sambatune_ui and start another instance, it will have a new password.</p> <p>NOTE: You will need to &gt; or use the kill command to stop sambatune_ui when you have finished. Not doing so will tie up the port. You can ps -elf | grep the_port_you_used to find the running processes. If you are not comfortable doing this, please ask for help."},{"location":"sambanova/SambaTune-User-Guide/#use-port-forwarding","title":"Use Port-Forwarding","text":"<p>This describes the steps to set up port-forwarding for applications, like SambaTune UI, which runs on the SambaNova system and binds to one or more ports. This example uses 8576 and 18576 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"sambanova/SambaTune-User-Guide/#from-your-local-machine","title":"From your local machine","text":"<p>This command sets up a port forward SambaNova login node to your local machine.</p> <p>Run</p> <pre><code>ssh -N -f -L localhost:18576:localhost:18576 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n</code></pre> <pre><code>#TODOBRW\nssh -v -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n\nssh wilsonb@sambanova.alcf.anl.gov\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"sambanova/SambaTune-User-Guide/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>This command sets up a port forward from a SambaNova node to the sambanova login machine.</p> <p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using that system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:18576:localhost:8576 ALCFUserID@sm-01\n</code></pre> <pre><code>#TODOBRW\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sm-01\n</code></pre>"},{"location":"sambanova/SambaTune-User-Guide/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:18576 on your local machine.</p> <p>Use the username and password from sm-01 to log in.</p>"},{"location":"sambanova/SambaTune-User-Guide/#ssh-notes","title":"SSH Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"sambanova/Steps-to-Run-BERT-Large-on-Sambanova-DataScale-SN10-8R/","title":"Steps to Run BERT-Large on Sambanova DataScale SN10-8R","text":"<ul> <li>BERT Code is in the Bert directory here for your reference.</li> <li>transformners_hook.py: contains code for BERT.</li> </ul>"},{"location":"sambanova/Steps-to-Run-BERT-Large-on-Sambanova-DataScale-SN10-8R/#pretraining-in-data-parallel-mode","title":"Pretraining in data parallel mode","text":"<p>Note: for the sake of the tutorial, we have precompiled the model and lowered the number of train steps to reduce the execution time.</p> <ol> <li>Create a folder for pretraining in your home repo, and copy the bash script <code>/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh</code> to it. Then, go to that folder. Example:</li> </ol> <pre><code>cd $HOME\nmkdir pretrain\ncp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh pretrain/\ncd pretrain/\n</code></pre> <ol> <li>Open the <code>submit-bert-pretrain-job-LBS1024.sh</code> file, and change <code>OUTDIR</code> to location of the pretrain folder. Example:</li> </ol> <pre><code>OUTDIR=$HOME/pretrain\n</code></pre> <p>Note: the per device batch size (LBS) is set to 1024 here. Also, the number of steps is set to 100, but this can be changed.</p> <ol> <li>SambaNova uses SLURM for job submission and queueing. We will use sbatch to submit our job to the job scheduler. Please refer to Sambanova Documentation for further details. In the following example, 2 RDUs are used:</li> </ol> <p><pre><code>sbatch --output=log_bert_pretrain_LBS1024_np2.out --gres=rdu:2 -c 8 submit-bert-pretrain-job-LBS1024.sh\n</code></pre>    Note: <code>-c</code> represents the number of cores per task</p> <ol> <li> <p>You can follow the status of your job using: <code>squeue</code>. The job should take about 8 min to complete.</p> </li> <li> <p>Once the job is completed, you can see the checkpoint(s) and accuracy metrics in <code>hf_output_lrg_run/</code>. The throughput is outputted in the <code>log_bert_pretrain_LBS1024_np2.out</code> file (search for throughput in the file).</p> <p> Click for sample throughput <pre><code>Measuring peformance with world size:  2\ninitial run starts.\ninitial run completes.\ne2e_latency: 30.75621747970581 seconds, throughput: 665.8816225861821 samples/s, measured over 10 iterations.\nNOTE: This is the combined throughput for 2 workers\ntotal duration: 30.75621747970581 s\n</code></pre> <p> Click for sample train_steps.txt <pre><code>10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n</code></pre> <p> Click for sample step_loss.txt <pre><code>11.16291\n10.76511\n10.44571\n10.16663\n9.98203\n9.85561\n9.76017\n9.66340\n9.57864\n9.50137\n</code></pre>"},{"location":"sambanova/Steps-to-Run-BERT-Large-on-Sambanova-DataScale-SN10-8R/#fine-tuning-for-question-answering-using-1-rdu","title":"Fine-tuning for question answering using 1 RDU","text":"<p>Note: for the sake of the tutorial, we have precompiled the model and lowered the number of train steps to reduce the execution time. We will also use a processed dataset.</p> <ol> <li>Create a folder for finetuning in your home repo, and copy the bash script <code>/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh</code> to it. Then, go to that folder. Example:</li> </ol> <pre><code>cd $HOME\nmkdir finetune\ncp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh finetune/\ncd finetune/\n</code></pre> <ol> <li> <p>Copy the processed dataset to the finetune repo. This will avoid tokenizing the dataset on the fly.    <pre><code>cp -r /projects/aitestbed_training/SN/precompiled_bert/squad_cache ./\n</code></pre></p> </li> <li> <p>Open the <code>submit-bert-squad-job.sh</code> file, and change <code>OUTDIR</code> to location of the finetune folder. Example:</p> </li> </ol> <p><pre><code>OUTDIR=$HOME/finetune\n</code></pre>    Note: the number of train epochs is set to 0.08, but this can be changed</p> <ol> <li>SambaNova uses SLURM for job submission and queueing. We will use sbatch to submit our job to the job scheduler. Please refer to Sambanova Documentation for further details. In the following example, 1 RDU is used:</li> </ol> <pre><code>sbatch --output=log_bert_squad.out --gres=rdu:1 -c 8 submit-bert-squad-job.sh\n</code></pre> <ol> <li> <p>You can follow the status of your job using: <code>squeue</code>. The job should take about 8 min to complete.</p> </li> <li> <p>Once the job is completed, you can see the checkpoint(s) and accuracy metrics in <code>hf_output_squad_run/</code>.</p> <p> Click for sample log_history.json <pre><code>[\n  {\n     \"exact\": 54.33301797540208,\n     \"f1\": 66.54507382283774,\n     \"epoch\": 0.07965242577842144,\n     \"total_flos\": 5419063617454080,\n     \"step\": 220\n   }\n]\n</code></pre> <p> Click for sample eval_results_squad.txt <pre><code>exact = 54.33301797540208\nf1 = 66.54507382283774\nepoch = 0.07965242577842144\ntotal_flos = 5419063617454080\n</code></pre>"},{"location":"sambanova/Steps-to-Run-BERT-Large-on-Sambanova-DataScale-SN10-8R/#other-models-and-use-cases","title":"Other Models and Use-cases","text":"<ul> <li>Full execution scripts (compile, run, measure-perf) for BERT-Large can be found under <code>/projects/aitestbed_training/SN/full_execution_bert/bash_scripts</code>.</li> <li><code>submit-bert-pretrain-job.sh</code>: bash script for pretraining job with 8 RDUs and LBS=256</li> <li> <p><code>submit-bert-squad-job.sh</code>: bash script for fine-tuning job for question answering with 1 RDU</p> </li> <li> <p>See Example Programs for instructions to run other well-known AI applications on SambaNova hardware (e.g., LeNet, FFN-MNIST, logistic regression, UNet)</p> </li> </ul>"},{"location":"sambanova/Steps-to-Run-a-Model-or-Program/","title":"Steps to Run a Model/Program","text":"<p>NOTE:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends.</p> <p>NOTE: Please use only Slurm commands, i.e., srun and sbatch, to run your code. If you run your code directly using the python command, it may cause conflicts on the system.</p>"},{"location":"sambanova/Steps-to-Run-a-Model-or-Program/#introduction","title":"Introduction","text":"<p>The SambaNova workflow includes the following main steps to run a model.</p> <ol> <li>Compile</li> <li>Run</li> <li>Test (optional)</li> </ol> <p>The system uses the Slurm job scheduler\u00a0to schedule the jobs and manage the workload on the system. For more information on Slurm, see Job Queueing and Submission.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"sambanova/Steps-to-Run-a-Model-or-Program/#compile","title":"Compile","text":"<p>Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware, how many compute and memory resources are required and how they will be used in all subsequent steps. The pef files are by default saved in the 'out' directory; the SambaNova documentation advises saving pef files in separate directories with the '--output-folder' option.</p> <p>It is necessary to re-compile only when the model changes, or parameters specific to the model graph change, including the batch size.  </p> <p>Compile times can be significant. Compile of the Unet sample, for example, when using images of size 32x32 pixels, takes 358 (s), and 1844 (s) for images of size 256x256.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre> <p>Where</p> Argument Default Help -b 1 Batch size for training"},{"location":"sambanova/Steps-to-Run-a-Model-or-Program/#run","title":"Run","text":"<p>This will run the application on SN nodes.</p> <pre><code>srun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>The location of the pef file generated in the compile step is passed as an argument to the run command.</p>"},{"location":"sambanova/Steps-to-Run-a-Model-or-Program/#test-optional","title":"Test (Optional)","text":"<p>This command is used to run the model on both the host CPU and the SambaNova node.  It compares the answers from the CPU and SambaNova RDU and will raise errors if any discrepancies are found. Pass the pef file generated as part of the compile step as the input to this command.</p> <pre><code>srun python lenet.py test --pef=\"pef/lenet/lenet.pef\"\n</code></pre>"},{"location":"sambanova/System-Overview/","title":"System Overview","text":""},{"location":"sambanova/System-Overview/#introduction","title":"Introduction","text":"<p>The SambaNova DataScale system is architected around the next-generation Reconfigurable Dataflow Unit (RDU) processor for optimal dataflow processing and acceleration. The AI Testbed's SambaNova system is a half-rack system consisting of two nodes, each of which features eight RDUs interconnected to enable model and data parallelism. SambaFlow, its software stack, extracts, optimizes, and maps dataflow graphs to the RDUs from standard machine learning frameworks, like PyTorch.</p> <p>Here is the link to the SambaNova white paper: Accelerated Computing with a Reconfigurable Dataflow Architecture</p>"},{"location":"sambanova/TODO/","title":"TODO","text":"<ul> <li> Simple how to run data parallel on SN.</li> <li> On first glance, It would be good to include information of what model is being run here.</li> <li> Can users just take an existing compiled model and run? No, they need to recompile here for this, right? Users coming from the GPU land would falter here otherwise.</li> <li> How is data parallel implemented in a single sentence would be good.</li> <li> What does intelligently split data mean? Is there a special or particular way it is done or can be done that is different from what is done, say, on a GPU or other systems today?</li> <li> What happens if I set ws = 3</li> <li> Why are we setting OMP_NUM_THREADS=8? What happens if we set it lower. If this is important, we should highlight it</li> <li> Mention that MPI is used to do data-parallel training. Number of MPI ranks should be equal to RDUs requested</li> <li> What does reduce on rdu imply. Please provide more details here.</li> <li> How does someone who has worked with Horovod or DDP move to use data parallel on SN?</li> <li> <p> Does bs=1 mean local batch.</p> </li> <li> <p> docs/sambanova/Best-Practices-and-FAQs.md ## MPI -- TODO -- this needs to be redone - may be part of data parallel page.</p> </li> <li> Edit Anatomy... to be a technical doc</li> <li> Edit DataParallel.md to be a technical doc</li> <li>[...] SN provide indexing for their docs. From training</li> <li> We need to include in the documentation where the SN docs are located,</li> <li> what the contents are,</li> <li> and how to view them.</li> <li> As part of what the contents are, we need some readme.md with the PDFs listed</li> <li> <p> SN PDF docs copy them locally. and then view them.  VV has a tar file also.</p> </li> <li> <p> Move snconfig from overview to Miscellaneous</p> </li> <li> Ask SN for a diagram.</li> <li> Use SN diagram.</li> </ul> <pre><code>## [SambaNova](https://github.com/argonne-lcf/ai-testbed-userdocs/tree/main/docs/sambanova)\n**DONE**1. [System Overview](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/System-Overview.md)\n    - **DONE** \"half-rack system\" ? **Yes**\n    - **DONE**Configuration section says\n        &gt; SambaNova node can be accessed as sm-01.ai.alcf.anl.gov. **This is the actual SN node.**\n\n       but page on [How to setup your base environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/How-to-setup-your-base-environment.md)says\n        ```bash\n        ssh ALCFUserID@sambanova.alcf.anl.gov\n        ```\n        **This is the log in node.**\n\n2. [How to setup your base environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/System-Overview.md)\n    **DONE**1. Page title reads like an instruction, whereas most of the others read like section titles **docs/sambanova/Logging-into-a-SambaNova-Node.md**\n    **SKIPPED**2. Could combine this page with [Using virtual environments to customize environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Using-virtual-environments-to-customize-environment.md) into a single **Environment Setup** page??\n\n3. [Using virtual environments to customize environment](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Using-virtual-environments-to-customize-environment.md)\n    **DONE** **As script**1. Might be worth mentioning what packages are included in `--system-site-packages`\n    2. What commonly used packages are missing / might need to be installed manually?\n4. [Steps to run a model / program](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Steps-to-run-a-model-or-program.md)\n    - Might be worth mentioning that the [Example Programs](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Example-Programs.md) page has instructions for making a local copy of the examples used in the documentation\n    - \"The SambaNova workflow includes the following main steps to run a model\"\n        - Maybe list steps explicitly and then expand on them in their respective sections? e .g.\n        - The SambaNova workflow includes the following main steps to run a model:\n            1. Compile\n            2. Run\n            3. Test (optional)\n        - Seems a bit unnecessary to list them as steps if the Test step is optional?\n    - **DONE**Where is the `pef` directory coming from in the `srun python lenet.py run --pef=\"pef/lenet/lenet.pef `command?\n\n\n11. [Miscellaneous](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Miscellaneous.md)\n    **VV seems fine with the original**1. Might be a good idea to include the links under Resources somewhere on the main page?\n12. \n\n- [ ] [Anatomy of SambaNova PyTorch Models](https://github.com/argonne-lcf/ai-testbed-userdocs/blob/main/docs/sambanova/Anatomy-of-SambaNova-PyTorch-Models.md)\n    1. Updates section references https://git.cels.anl.gov/ai-testbed-apps/sambanova/sambanova_starter.git but that link requires special permission to view\n\n\n- [X] **There is already a link to the Example Programs.**Page Steps to Run a Model/Program : Add \u201c cd ~/apps/starters/\u201c before the example \n- [X] **I would have to set up a new repo from SC22 paper.** Page Performance Tools : Measure TFLOPs : Conv2D forward pass example file location can be provided\n- [X] Page Example Programs : section UNet : Remove question mark here --batch-size=1? python unet.py run --do-train  --in-channels=3  --in-width=32  --in-height=32 --init-features 32 --batch-size=1? --data-dir $DATADIR --log-dir ${OUTDIR}/log_dir_unet32_train --epochs 5 --pef=${OUTDIR}/unet_train/unet_train.pef\n\nPage Best Practices and FAQs : Section Data Parallel : Add unet_main.py file location\n- [X] Page Best Practices and FAQs : Section Data Parallel : Remove \u201c- \u2014\u201c near python unet_main.py compile - --batch-size=48\n- [X] Page Best Practices and FAQs : Section Data Parallel : Remove \u201c- \u2014\u201c  unet_main.py run - -p . \n- [Doc'd elsewhere] Also set export - [ SOFTWARE_HOME before compile and add to \u2014pef-name \n- [X] Argonne SambaNova-Training-June2021 and Human Decisions Files notes link is broken \n\nTypos : Page :Anatomy of SambaNova PyTorch Model  typo at\n- [X] \u201cThen once every 10,000 epics, print the status\u201d\n- [X] \u201cContains the train and test methods and a littile more.\u201d\n- [X] \u201cinstantiated instantiate\u201d\n- [X] \u201cthe the bigger systems \u201c\n- [X] \u201ct the the various \u201c\n\nPage Best Practices and FAQs: \n- [UTL] \u201ct and environment variable\u201d \n- [X] \u201cwhere the loss is calculating across - calculated\u201c \n\n- [X] Page Steps to Run a Model/Program: \u201cTest (Optional) This commands - command\u201c\n- [X] Page How to Setup Your Base Environment - \u201c request an acccount a\u201d \u201cgenerted - generated\u201d \n\n\n- [X] ModelZoo to /software/sambanova/apps/ and also use symbolic link for latest.\n\n- [X] How to do DataParallel.  add page.  See Confluence and maybe video and its .pptx\n\n- [X] How to spec 2 or more RDUs on SN?  Queue and Sub also.  \n- [model parallel] can do in compile phase v. run can be different.\n\n- [X] Move SN starter code to /software/sambanova/tutorials\n\n- [OK] just fyi, here's how to serve html from the bastion node (e.g. cerebras.alcf.anl.gov):\n\n```bash\nssh -t -L localhost:8089:localhost:8089 arnoldw@cerebras.alcf.anl.gov  \"cd /software/cerebras/docs/V1.1/;python3 -m http.server 8089\"\n</code></pre>"},{"location":"sambanova/TODO/#system-overview","title":"System Overview","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#how-to-setup-your-base-environment","title":"How to setup your base environment","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/18</li> <li> Second draft reviewed</li> <li> Third draft 4/20</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#using-virtual-environments-to-customize-environment","title":"Using Virtual Environments to Customize Environment","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/18</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#steps-to-run-a-modelprogram","title":"Steps to Run a Model/Program","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/19</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#job-queueing-and-submission","title":"Job Queueing and Submission","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/18</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#example-programs","title":"Example Programs","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft 4/19</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#performance-tools","title":"Performance Tools","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#best-practices-and-faqs","title":"Best Practices and FAQs","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#tunneling-and-forwarding-ports","title":"Tunneling and Forwarding Ports","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#system-and-storage-policies","title":"System and Storage Policies","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#miscellaneous","title":"Miscellaneous","text":"<ul> <li> First draft 4/19</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/TODO/#anatomy-of-sambanova-pytorch-models","title":"Anatomy of SambaNova PyTorch Models","text":"<ul> <li> First draft</li> <li> First draft reviewed</li> <li> Second draft</li> <li> Second draft reviewed</li> <li> Third draft</li> <li> Third draft reviewed</li> <li> Final draft</li> <li> Final draft reviewed</li> </ul>"},{"location":"sambanova/Tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>Port forwarding is covered here.  This is specifically for TensorBoard.</p>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#tensorboard-port-forwarding","title":"TensorBoard Port-Forwarding","text":"<p>This section describes the steps to be followed to set up port forwarding for applications, like TensorBoard, which runs on the SambaNova system and binds to one or more ports. This example uses 6006 and 16006 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#from-your-local-machine","title":"From your local machine","text":"<p>Run</p> <pre><code>ssh -v -N -f -L localhost:16006:localhost:16006 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilPass+ code &gt;\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using the appropriate system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:16006:localhost:6006 ALCFUserID@sm-01\nssh ALCFUserID@sm-01\n</code></pre>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#on-sm-01","title":"On sm-01","text":"<p>Execute the following command:</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>Navigate to the appropriate directory for your model. Launch your model using srun or sbatch.</p> <pre><code>cd /path/to/your/project\nsbatch --output=pef/my_model/output.log submit-my_model-job.sh\n</code></pre>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#on-another-sm-01-terminal-window","title":"On Another sm-01 Terminal Window","text":"<p>The SambaNova system has a bash shell script to setup the required software environment. This sets up the SambaFlow software stack, the associated environmental variables and activates a pre-configured virtual environment.</p> <p>Use</p> <pre><code>ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh\n(venv) ALCFUserID@sm-01:~$\n</code></pre> <p>Navigate to the appropriate directory for your model.</p> <pre><code>cd /path/to/your/project\ntensorboard --logdir /logs --port 6006\n</code></pre>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:16006 on your local machine.</p>"},{"location":"sambanova/Tunneling-and-forwarding-ports/#notes","title":"Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"sambanova/Virtual-Environment/","title":"Virtual Environments to Customize Environment","text":""},{"location":"sambanova/Virtual-Environment/#using-a-virtual-venv","title":"Using a Virtual Venv","text":"<p>To create a virtual environment, one can use the --system-site-packages flag:</p> <pre><code>python -m venv --system-site-packages my_env\nsource my_env/bin/activate\n</code></pre>"},{"location":"sambanova/Virtual-Environment/#system-site-packages","title":"System Site Packages","text":"<p>There are many packages available on the system. Run the following Python script to retrieve the location of the packages:</p> <pre><code>import sys\nsite_packages = next(p for p in sys.path if 'site-packages' in p)\nprint(site_packages)\n</code></pre> <p>Given the location of the packages, one may list the packages. For example:</p> <pre><code>ls -al /opt/sambaflow/venv/lib/python3.7/site-packages\n</code></pre>"},{"location":"sambanova/Virtual-Environment/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install \"SomeProject\"\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre> <p>Note: Conda is not supported on the SambaNova system.</p>"},{"location":"sambanova/getting-started/","title":"Getting Started","text":""},{"location":"sambanova/getting-started/#on-boarding","title":"On-Boarding","text":"<p>See Get Started to request an account and additional information.</p>"},{"location":"sambanova/getting-started/#setup","title":"Setup","text":""},{"location":"sambanova/getting-started/#system-view","title":"System View","text":"<p>Connection to a SambaNova node is a two-step process. The first step is to ssh into the login node. This step requires an MFA passcode for authentication - an eight-digit passcode generated by an app on your mobile device, e.g., MobilePASS+. The second step is to log in to a SambaNova node from the login node.</p> <p>The SambaNova system has four racks, r1..4 and two hosts, h1..2, per rack.  So, sn30-r2-h1 is the node on rack 2 and host 1.</p> <p></p>"},{"location":"sambanova/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Log in to the SambaNova login node from your local machine using the below command. This uses the MobilPass+ token generated every time you log in to the system. This is the same passcode used to authenticate into other ALCF systems, such as Theta and Cooley.</p> <p>In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nPassword: &lt; MobilPass+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"sambanova/getting-started/#log-in-to-a-sambanova-node","title":"Log in to a SambaNova Node","text":"<p>Once you are on the login node, the SambaNova system can be accessed using one of the following aliases:</p> <pre><code>sn30-r1-h1\nsn30-r1-h2\n\nsn30-r2-h1\nsn30-r2-h2\n\nsn30-r3-h1\nsn30-r3-h2\n\nsn30-r4-h1\nsn30-r4-h2\n</code></pre> <p>Example:</p> <pre><code>ssh sn30-r1-h1\n</code></pre>"},{"location":"sambanova/getting-started/#sdk-setup","title":"SDK setup","text":"<p>With SambaFlow version 1.14, the base SambaFlow python environment is installed as part of the system, and the user path is updated automatically to include the directories for the sambanova binaries. Use of python virtual environments made with e.g. <code>virtualenv</code> is recommended. Conda is not supported. See Virtual Environment for details.</p> <p>&lt;!--</p>"},{"location":"sambanova/getting-started/#starters","title":"Starters","text":"<pre><code>source /opt/sambaflow/apps/starters/mlp/venv/bin/activate\nsource /opt/sambaflow/apps/starters/lenet/venv/bin/activate\nsource /opt/sambaflow/apps/starters/ffn_mnist/venv/bin/activate\nsource /opt/sambaflow/apps/starters/logreg/venv/bin/activate\nsource /opt/sambaflow/apps/starters/upscalenet/venv/bin/activate\nsource /opt/sambaflow/apps/starters/power_pca/venv/bin/activate\n</code></pre>"},{"location":"sambanova/getting-started/#images","title":"Images","text":"<pre><code>source /opt/sambaflow/apps/image/segmentation_3d/venv/bin/activate\nsource /opt/sambaflow/apps/image/deepvit/venv/bin/activate\nsource /opt/sambaflow/apps/image/segmentation/venv/bin/activate\nsource /opt/sambaflow/apps/image/object_detection/venv/bin/activate\nsource /opt/sambaflow/apps/image/classification/venv/bin/activate\n</code></pre>"},{"location":"sambanova/getting-started/#recommenders","title":"Recommenders","text":"<pre><code>source /opt/sambaflow/apps/recommender/dlrm/venv/bin/activate\nsource /opt/sambaflow/apps/recommender/ncf/venv/bin/activate\nsource /opt/sambaflow/apps/recommender/deepinterest/venv/bin/activate\n</code></pre>"},{"location":"sambanova/getting-started/#nlp","title":"NLP","text":"<pre><code>source /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/bin/activate\nsource /opt/sambaflow/apps/nlp/transformers_on_rdu/gpt13b/venv/bin/activate\nsource /opt/sambaflow/apps/nlp/data_processing/venv/bin/activate\n</code></pre>"},{"location":"sambanova/getting-started/#other","title":"Other","text":"<pre><code>source /opt/sambaflow/apps/private/anl/venv/bin/activate\nsource /opt/sambaflow/apps/micros/venv/bin/activate\n</code></pre> <p>You may deactivate the environment if finished.</p> <pre><code>deactivate\n</code></pre>"},{"location":"sambanova/readme-rick/","title":"SambaTune","text":""},{"location":"sambanova/readme-rick/#notes","title":"Notes","text":"<pre><code>#TODOBRW\nssh wilsonb@homes.cels.anl.gov\nssh sm-02\nMobilePass+ password\nOn sm-02\nsource /opt/sambaflow/venv/bin/activate\nsambatune_ui --directory /home/wilsonb/tmp/sambatune_gen --port 8580\n#There will be a username and password displayed that you will use in your browser on your laptop.\nCommand used on laptop for port forward\nssh -XL 8580:127.0.0.1:8580 wilsonb@sm-02.cels.anl.gov\nMobilePass+ password\n# You will be logged into sm-02 but, you do not need to do anything.\naddress used in browser on laptop localhost:8580\n#Use username and password from sambatune_ui.\nUsername\nPassword\n</code></pre>"},{"location":"sambanova/readme-rick/#rick","title":"Rick","text":"<p>8/24/2022</p> <p>I have updated ~rweisner/tmp/sambatune with sambatune_ui 1.1 and updated the readme.</p>"},{"location":"sambanova/readme-rick/#about-sambatune","title":"About SambaTune","text":"<p>SambaTune is a tool for profiling, debugging, and tuning performance of applications running on SN hardware.</p> <p>The tool automates collection of hardware performance counters, metrics aggregation, report generation, and visualization. It also automates benchmarking of the application to compute average throughput over a sufficient number of runs. The tool is designed to aid the user with performance bottleneck analysis and tuning.</p> <p>SambaTune is currently used by SN engineers involved in performance tuning efforts. SambaTune is also planned for release to external customers to aid with performance bottleneck analysis and resolution.</p>"},{"location":"sambanova/readme-rick/#installation","title":"Installation","text":"<pre><code>ssh wilsonb@sambanova.alcf.anl.gov\nMobilePass+ pwd\nssh sm-01\n</code></pre> <p>First, enter the virtual environment on sm-01 or sm-02:</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre>"},{"location":"sambanova/readme-rick/#usage","title":"Usage","text":"<pre><code>usage: sambatune [-h] [--artifact-root ARTIFACT_ROOT] [--disable-override]\n[--compile-only | -m MODES [MODES ...]]\n[--version]\nconfig\npositional arguments:\nconfig\nYAML file with model, compile, run configuration.\noptional arguments:\n-h, --help\n--artifact-root\nshow this help message and exit\nARTIFACT_ROOT\nCustom location to save compile/run artifacts;\ndefaults to '$DUMP_ROOT/artifact_root'\n--disable-override Reuse the placement from the baseline compilation\n--compile-only Run compilation of PEFs for selected modes only\n-m MODES [MODES ...], --modes MODES [MODES ...]\nSelect modes to execute from ['benchmark',\n'instrument', 'run'] default: ['benchmark']\n--version\nversion of sambatune and sambaflow.\n</code></pre>"},{"location":"sambanova/readme-rick/#command-overview","title":"Command Overview","text":"<p>By default, it will run with the benchmarking mode enabled. Use the --modes flag to run modes individually or in any combination. Benchmark-Only:</p> <pre><code>sambatune small_vae.yaml --artifact_root $(pwd)/artifact_root --modes benchmark\n</code></pre> <p>Instrument-Only:</p> <pre><code>sambatune small_vae.yaml --artifact_root $(pwd)/artifact_root --modes instrument\n</code></pre> <p>All modes:</p> <pre><code>sambatune small_vae.yaml --artifact_root $(pwd)/artifact_root --modes instrument\n</code></pre>"},{"location":"sambanova/readme-rick/#command-example","title":"Command Example","text":""},{"location":"sambanova/readme-rick/#running","title":"Running","text":"<p>Run the following example on sm-01 or sm-02:</p> <pre><code>mkdir ~/sambatune\ncd ~/sambatune\nsambatune small_vae.yaml --artifact_root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>where small_vae.yaml is a user-specified configuration file:</p>"},{"location":"sambanova/readme-rick/#samples-config-file","title":"Samples Config File","text":"<p>The current directory should be ~/sambatune.</p> <p>Create small_vae.yaml with the following content using your favorite editor.</p> <pre><code>small_vae.yaml:\napp: /opt/sambaflow/apps/private/anl/moleculevae.py\n\nmodel-args: -b 128 --in-width 512 --in-height 512\n\ncompile-args: compile --plot --enable-conv-tiling --compiler-configs-file /opt/sambaflow/apps/private/anl/moleculevae/compiler_configs_conv.json --mac-v2 --mac-human-decision /opt/sambaflow/apps/private/anl/moleculevae/symmetric_human_decisions_tiled_v2.json\n\nrun-args: --num-iterations 1000 --input-path /var/tmp/dataset/moleculevae/ras1_prot-pops.h5 --out-path ${HOME}/moleculevae_out --model-id 0 --epochs 10\n\nenv:\n     OMP_NUM_THREADS: 16\n     SF_RNT_FSM_POLL_BUSY_WAIT: 1\n     SF_RNT_DMA_POLL_BUSY_WAIT: 1\n     CONVFUNC_DEBUG_RUN: 0\n</code></pre>"},{"location":"sambanova/readme-rick/#install-sambatune-ui-on-your-development-machine","title":"Install SambaTune UI on Your Development Machine","text":""},{"location":"sambanova/readme-rick/#copy-conda-tar-file-on-sambanova","title":"Copy Conda Tar File on SambaNova","text":"<p>On sambanova.alcf.anl.gov:</p> <pre><code>mkdir ~/tmp\ncd ~/tmp\ncp /home/rweisner/tmp/sambatune/sambatune_1.1.tar .\n</code></pre>"},{"location":"sambanova/readme-rick/#copy-conda-tar-file-to-your-dev-machine","title":"Copy Conda Tar File To Your Dev Machine","text":"<p>On your dev machine:</p> <pre><code>mkdir /tmp\ncd /tmp\nscp ALCFUserID@sambanova:tmp/sambatune/sambatune_1.1.tar .\n# Or\nscp ac.rick.weisner@lambda0:tmp/sambatune/sambatune_1.1.tar .\n# Or\nscp wilsonb@sambanova:tmp/sambatune/sambatune_1.1.tar .\n</code></pre>"},{"location":"sambanova/readme-rick/#install-docker","title":"Install Docker","text":"<p>If necessary:</p> <pre><code>sudo apt-get install docker\n# Or\nsudo snap install docker\n</code></pre>"},{"location":"sambanova/readme-rick/#docker","title":"Docker","text":"<p>If you have changed directories:</p> <pre><code>cd /tmp\n</code></pre> <p>Load Docker image:</p> <pre><code>sudo docker image load -i sambatune_1.1.tar\n</code></pre> <p>List Docker images:</p> <pre><code>sudo docker image ls\n</code></pre> <p>Your output will look something like:</p> <pre><code>REPOSITORY                                                                                      TAG       IMAGE ID       CREATED         SIZE\nartifacts.sambanovasystems.com/sustaining-docker-lincoln-dev/sambatune/sambatune-client         1.1       bf1d5834776d   3 months ago    737MB\n</code></pre> <p>This is the image you want artifacts.sambanovasystems.com/sustaining-docker-lincoln-dev/sambatune/sambatune-client         1.1       bf1d5834776d   3 months ago    737MB</p>"},{"location":"sambanova/readme-rick/#run-the-docker-container","title":"Run the Docker Container","text":"<p>Make a work directory:</p> <pre><code>mkdir -p /path/to/work\n# Or\nmkdir -p /home/bwilson/sambatune/work\n</code></pre> <p>Run the container:</p> <pre><code>sudo docker container run --mount type=bind,source=/path/to/work,target=/work -it  -p 5050:8576 artifacts.sambanovasystems.com/sustaining-docker-lincoln-dev/sambatune/sambatune-client:1.1\n# Or\nsudo docker container run --mount type=bind,source=/home/bwilson/sambatune/work,target=/work -it  -p 5050:8576 artifacts.sambanovasystems.com/sustaining-docker-lincoln-dev/sambatune/sambatune-client:1.1\n</code></pre> <p>The first time you run the above command, you will see many layers being loaded.  It will load immediate from then on.</p> <p>My artifact_root is in /Users/rickw/work/vae_tst/artifact_root.</p> <p>Start the UI: It will tell you the port and password.</p> <p>sambatune_ui --directory /work/lincoln/vae_tst/artifact_root/sambatune_gen</p> <p>You will see something like: root@477a49bd9e55:/project# sambatune_ui --directory /work/lincoln/vae_tst/artifact_root/sambatune_gen Starting server on localhost:8576         with the following directory: /work/lincoln/vae_tst/artifact_root/sambatune_gen with the,          username: \"admin\", password: \"fd11af8a-edad-11ec-89c9-0242ac110002\"  * Serving Flask app 'sambatune.uiwebserver' (lazy loading)  * Environment: production    WARNING: This is a development server. Do not use it in a production deployment.    Use a production WSGI server instead.  * Debug mode: off  * Running on all addresses.    WARNING: This is a development server. Do not use it in a production deployment.  * Running on http://172.17.0.2:8576/ (Press CTRL+C to quit)</p> <p>RCW: use localhost:8576 to connect</p> <p>Now connect via browser.</p>"},{"location":"sambanova/readme/","title":"SambaNova Documentation","text":"<p>compiler-options.pdf getting-started.pdf intro-tutorial-pytorch.pdf release-notes.pdf run-examples-language.pdf run-examples-pytorch.pdf run-examples-vision.pdf runtime-faq.pdf slurm-sambanova.pdf snconfig-userguide.pdf sntilestat-manpage.pdf using-layernorm.pdf using-venvs.pdf</p>"},{"location":"sambanova/readme_rick_02/","title":"Notes","text":"<pre><code>source /software/sambanova/envs/sn_env.sh\nsource ~/.bashrc\ncd ~/tmp\ncp -rf /home/rweisner/tmp/unet .\ncd ~/tmp/unet\nexport OUTDIR=~/apps/image/unet\nexport DATADIR=/software/sambanova/dataset/kaggle_3m\nsbatch --gres=rdu:1 --tasks-per-node 4  --nodes 2 --nodelist sm-02,sm-01 --cpus-per-task=16 ./unet_batch.sh ${NP} ${NUM_WORKERS}\n./unet_compile_run_all.sh compile 256 256\nll\n</code></pre> <pre><code>On sm-01 or sm-02\n/home/rweisner/tmp/gpt for mpirun\n/home/rweisner/tmp/unet\nunet_compile_run_all.sh for slurm\n./unet_compile_run_all.sh compile 256 256\n</code></pre>"},{"location":"sambanova/running-a-model-or-program/","title":"Steps to Run a Model/Program","text":"<p>NOTE:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends.</p> <p>NOTE: Please use only Slurm commands, i.e., srun and sbatch, to run your code. If you run your code directly using the python command, it may cause conflicts on the system.</p>"},{"location":"sambanova/running-a-model-or-program/#introduction","title":"Introduction","text":"<p>The SambaNova workflow includes the following main steps to run a model.</p> <ol> <li>Compile</li> <li>Run (train)</li> <li>Test (optional)</li> </ol> <p>The system uses the Slurm job scheduler\u00a0to schedule the jobs and manage the workload on the system. For more information on Slurm, see Job Queueing and Submission.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"sambanova/running-a-model-or-program/#compile","title":"Compile","text":"<p>Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware, how many compute and memory resources are required, and how they will be used in all subsequent steps. The pef files are by default saved in the 'out' directory; the SambaNova documentation advises saving pef files in separate directories with the '--output-folder' option.</p> <p>It is necessary to re-compile only when the model changes, or parameters specific to the model graph change, including the batch size.</p> <p>Compile times can be significant. Compile of the Unet3D sample, for example, when using images of size 128x128x128 pixels, takes as long as 1400 seconds.</p> <p>Example:</p> <pre><code>source /opt/sambaflow/apps/starters/lenet/venv\nsrun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre> <p>Where</p> Argument Default Help -b 1 Batch size for training"},{"location":"sambanova/running-a-model-or-program/#run","title":"Run","text":"<p>This will run the application on SN nodes.</p> <pre><code>srun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>The location of the pef file generated in the compile step is passed as an argument to the run command.</p>"},{"location":"sambanova/running-a-model-or-program/#test-optional","title":"Test (Optional)","text":"<p>This command is used to run the model on both the host CPU and the SambaNova node.  It compares the answers from the CPU and SambaNova RDU and will raise errors if any discrepancies are found. Pass the pef file generated as part of the compile step as the input to this command.</p> <pre><code>srun python lenet.py test --pef=\"pef/lenet/lenet.pef\"\n</code></pre>"},{"location":"sambanova/running-bert-large-on-sn10-8r/","title":"Steps to Run BERT-Large on Sambanova DataScale SN10-8R","text":"<ul> <li>BERT Code is in the Bert directory here for your reference.</li> <li>transformners_hook.py: contains code for BERT.</li> </ul>"},{"location":"sambanova/running-bert-large-on-sn10-8r/#pretraining-in-data-parallel-mode","title":"Pretraining in data parallel mode","text":"<p>Note: for the sake of the tutorial, we have precompiled the model and lowered the number of train steps to reduce the execution time.</p> <ol> <li>Create a folder for pretraining in your home repo, and copy the bash script <code>/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh</code> to it. Then, go to that folder. Example:</li> </ol> <pre><code>cd $HOME\nmkdir pretrain\ncp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh pretrain/\ncd pretrain/\n</code></pre> <ol> <li>Open the <code>submit-bert-pretrain-job-LBS1024.sh</code> file, and change <code>OUTDIR</code> to location of the pretrain folder. Example:</li> </ol> <pre><code>OUTDIR=$HOME/pretrain\n</code></pre> <p>Note: the per device batch size (LBS) is set to 1024 here. Also, the number of steps is set to 100, but this can be changed.</p> <ol> <li>SambaNova uses SLURM for job submission and queueing. We will use sbatch to submit our job to the job scheduler. Please refer to Sambanova Documentation for further details. In the following example, 2 RDUs are used:</li> </ol> <p><pre><code>sbatch --output=log_bert_pretrain_LBS1024_np2.out --gres=rdu:2 -c 8 submit-bert-pretrain-job-LBS1024.sh\n</code></pre>    Note: <code>-c</code> represents the number of cores per task</p> <ol> <li> <p>You can follow the status of your job using: <code>squeue</code>. The job should take about 8 min to complete.</p> </li> <li> <p>Once the job is completed, you can see the checkpoint(s) and accuracy metrics in <code>hf_output_lrg_run/</code>. The throughput is outputted in the <code>log_bert_pretrain_LBS1024_np2.out</code> file (search for throughput in the file).</p> <p> Click for sample throughput <pre><code>Measuring peformance with world size:  2\ninitial run starts.\ninitial run completes.\ne2e_latency: 30.75621747970581 seconds, throughput: 665.8816225861821 samples/s, measured over 10 iterations.\nNOTE: This is the combined throughput for 2 workers\ntotal duration: 30.75621747970581 s\n</code></pre> <p> Click for sample train_steps.txt <pre><code>10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n</code></pre> <p> Click for sample step_loss.txt <pre><code>11.16291\n10.76511\n10.44571\n10.16663\n9.98203\n9.85561\n9.76017\n9.66340\n9.57864\n9.50137\n</code></pre>"},{"location":"sambanova/running-bert-large-on-sn10-8r/#fine-tuning-for-question-answering-using-1-rdu","title":"Fine-tuning for question answering using 1 RDU","text":"<p>Note: for the sake of the tutorial, we have precompiled the model and lowered the number of train steps to reduce the execution time. We will also use a processed dataset.</p> <ol> <li>Create a folder for finetuning in your home repo, and copy the bash script <code>/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh</code> to it. Then, go to that folder. Example:</li> </ol> <pre><code>cd $HOME\nmkdir finetune\ncp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh finetune/\ncd finetune/\n</code></pre> <ol> <li> <p>Copy the processed dataset to the finetune repo. This will avoid tokenizing the dataset on the fly.    <pre><code>cp -r /projects/aitestbed_training/SN/precompiled_bert/squad_cache ./\n</code></pre></p> </li> <li> <p>Open the <code>submit-bert-squad-job.sh</code> file, and change <code>OUTDIR</code> to location of the finetune folder. Example:</p> </li> </ol> <p><pre><code>OUTDIR=$HOME/finetune\n</code></pre>    Note: the number of train epochs is set to 0.08, but this can be changed</p> <ol> <li>SambaNova uses SLURM for job submission and queueing. We will use sbatch to submit our job to the job scheduler. Please refer to Sambanova Documentation for further details. In the following example, 1 RDU is used:</li> </ol> <pre><code>sbatch --output=log_bert_squad.out --gres=rdu:1 -c 8 submit-bert-squad-job.sh\n</code></pre> <ol> <li> <p>You can follow the status of your job using: <code>squeue</code>. The job should take about 8 min to complete.</p> </li> <li> <p>Once the job is completed, you can see the checkpoint(s) and accuracy metrics in <code>hf_output_squad_run/</code>.</p> <p> Click for sample log_history.json <pre><code>[\n  {\n     \"exact\": 54.33301797540208,\n     \"f1\": 66.54507382283774,\n     \"epoch\": 0.07965242577842144,\n     \"total_flos\": 5419063617454080,\n     \"step\": 220\n   }\n]\n</code></pre> <p> Click for sample eval_results_squad.txt <pre><code>exact = 54.33301797540208\nf1 = 66.54507382283774\nepoch = 0.07965242577842144\ntotal_flos = 5419063617454080\n</code></pre>"},{"location":"sambanova/running-bert-large-on-sn10-8r/#other-models-and-use-cases","title":"Other Models and Use-cases","text":"<ul> <li>Full execution scripts (compile, run, measure-perf) for BERT-Large can be found under <code>/projects/aitestbed_training/SN/full_execution_bert/bash_scripts</code>.</li> <li><code>submit-bert-pretrain-job.sh</code>: bash script for pretraining job with 8 RDUs and LBS=256</li> <li> <p><code>submit-bert-squad-job.sh</code>: bash script for fine-tuning job for question answering with 1 RDU</p> </li> <li> <p>See Example Programs for instructions to run other well-known AI applications on SambaNova hardware (e.g., LeNet, FFN-MNIST, logistic regression, UNet)</p> </li> </ul>"},{"location":"sambanova/sambanova/","title":"SambaNova","text":""},{"location":"sambanova/sambanova/#pytorch-mirrors","title":"PyTorch Mirrors","text":"<p>See\u00a0https://github.com/pytorch/examples .</p> <p>There are two mirrors (in the python docs) used for downloading the mnist dataset.</p> <p>mirrors = [ \u00a0 \u00a0 \u00a0 \u00a0 'http://yann.lecun.com/exdb/mnist/', \u00a0 \u00a0 \u00a0 \u00a0 'https://ossci-datasets.s3.amazonaws.com/mnist/']</p> <p>yann.lecun.com appears to be intermittently broken (503 errors).</p>"},{"location":"sambanova/sambanova/#resources","title":"Resources","text":"<ul> <li> <p>https://docs.ai.alcf.anl.gov/sambanova/</p> </li> <li> <p>Argonne SambaNova Training   11/20</p> </li> <li> <p>https://docs.sambanova.ai\u00a0Create a   SambaNova account if you do not have one.</p> </li> <li> <p>Getting Started with   SambaFlow   Skip this one.</p> </li> <li> <p>Tutorial: Creating Models with   SambaFlow</p> </li> <li> <p>Administrators -- @ryade</p> </li> </ul>"},{"location":"sambanova/sambanova/#further-information","title":"Further Information","text":"<p>Human Decisions Files notes</p>"},{"location":"sambanova/sambanova/#creating-a-sambanova-portal-account-to-access-the-documentation-portal","title":"Creating a SambaNova Portal Account to access the documentation portal","text":"<ol> <li> <p>Go to\u00a0\u00a0login.sambanova.ai;</p> </li> <li> <p>Select the \"Sign up\" link at the bottom;</p> </li> <li> <p>Enter your information</p> <ol> <li> <p>Your ANL email address;</p> </li> <li> <p>A password that you choose to access the site;</p> </li> <li> <p>First name;</p> </li> <li> <p>Last name;</p> </li> <li> <p>Alternate email address;</p> </li> <li> <p>Use\u00a064693137 for the CLOUD ID;</p> </li> <li> <p>Select \"Register\" button;</p> </li> <li> <p>Note: The new web page may be displaying a QR code.\u00a0 Do not navigate away from it.\u00a0 Please edit this page to describe what happenes for you.</p> </li> </ol> </li> <li> <p>Verify your email address</p> <ol> <li> <p>Open your ANL email;</p> </li> <li> <p>Open the email from Okta;</p> </li> <li> <p>Select the \"Activate Account\" button;</p> </li> <li> <p>Select the \"Configure factor\" button on the displayed web page;</p> </li> <li> <p>Select either iPhone or Android for the device time on the new web page;</p> </li> <li> <p>Install Okta Verify from the App Store/Google Play Store onto your mobile device.;</p> </li> <li> <p>Select \"Next\" button on the web page;</p> </li> </ol> </li> <li> <p>On your phone</p> <ol> <li> <p>Open Okta Verify app;</p> </li> <li> <p>Select \"Get Started\" button;</p> </li> <li> <p>Select \"Next\" button;</p> </li> <li> <p>Select \"Add Account\" button;</p> </li> <li> <p>Select \"Organization\" for Account Type;</p> </li> <li> <p>Scan the QR Code shown in the browser;</p> </li> </ol> </li> <li> <p>Sign in to the SambaNova web site</p> <ol> <li>Select the \"SambaNova Documentation\" button.</li> </ol> </li> </ol> <p>Authorization for sections of the SambaNova site uses the tuple (email address, cloud id). For ANL users, these should be an anl email address and the cloud id specified above (64693137). (Note: the cloud id can be changed in the SambaNova user settings.) *If you are not at Argonne, please send us an email (ai@alcf.anl.gov) for access.\u00a0**</p> <p>If you plan to publish, say to a conference, workshop or journal, we have a review process wherein you share the draft with us (pre-submission) at\u00a0ai@alcf.anl.gov\u00a0and we will\u00a0work\u00a0with\u00a0SambaNova\u00a0for the\u00a0requisite\u00a0approvals.</p>"}]}