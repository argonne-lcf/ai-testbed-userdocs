# Example Programs

## Copy Examples

Copy starters to your personal directory structure:

```bash
cd ~/
cp -r /software/sambanova/apps/ .
```

## LeNet

Change directory

```bash
cd ~/apps/starters/
```

## Common Arguments

Arguments

| Argument               | Default   | Help                           |
|------------------------|-----------|--------------------------------|
| -b                     | 1         | Batch size for training        |
|                        |           |                                |
| -n,                    | 100       | Number of iterations to run    |
| --num-iterations       |           | the pef for                    |
|                        |           |                                |
| -e,                    | 1         | Number epochs for training     |
| --num-epochs           |           |                                |
|                        |           |                                |
| --log-path             | 'check    | Log path                       |
|                        | points'   |                                |
|                        |           |                                |
| --num-workers          | 0         | Number of workers              |
|                        |           |                                |
| --measure-train-       | None      | Measure training performance   |
| performance            |           |                                |
|                        |           |                                |

### LeNet Arguments

This is not an exhaustive list of arguments.

Arguments

| Argument               | Default   | Help                           |
|------------------------|-----------|--------------------------------|
| --lr                   | 0.01      | Learning rate for training     |
|                        |           |                                |
| --momentum             | 0.0       | Momentum value for training    |
|                        |           |                                |
| --weight-decay         | 0.01      | Weight decay for training      |
|                        |           |                                |
| --data-path            | './data'  | Data path                      |
|                        |           |                                |
| --data-folder          | 'mnist_   | Folder containing mnist data   |
|                        | data'     |                                |
|                        |           |                                |

**NOTE:  If you receive an "HTTP error" message on any of the
following commands, run the command again. Such errors (e.g 503) are
commonly an intermittent failure to download a dataset.**

Run these commands:

```bash
srun python lenet.py compile -b=1 --pef-name="lenet" --output-folder="pef"
srun python lenet.py test --pef="pef/lenet/lenet.pef"
srun python lenet.py run --pef="pef/lenet/lenet.pef"
srun python lenet.py measure-performance --pef="pef/lenet/lenet.pef"
```

To use Slurm sbatch, create submit-lenet-job.sh with the following
contents:

```bash
!/bin/sh

python lenet.py compile -b=1 --pef-name="lenet" --output-folder="pef"
python lenet.py test --pef="pef/lenet/lenet.pef"
python lenet.py run --pef="pef/lenet/lenet.pef"
python lenet.py measure-performance --pef="pef/lenet/lenet.pef"
```

Then

```bash
sbatch --output=pef/lenet/output.log submit-lenet-job.sh
```

Squeue will give you the queue status.

```bash
squeue
```

The output file, pef/lenet/output.log, will look something like:

```text
[Info][SAMBA][Default] # Placing log files in
pef/lenet/lenet.samba.log

[Info][MAC][Default] # Placing log files in
pef/lenet/lenet.mac.log

[Warning][SAMBA][Default] #

--------------------------------------------------

Using patched version of torch.cat and torch.stack

--------------------------------------------------

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen

[Warning][MAC][GraphLoweringPass] # lenet__reshape skip
set_loop_to_air

[Warning][MAC][GraphLoweringPass] # lenet__reshape_bwd skip
set_loop_to_air

...

Epoch [1/1], Step [59994/60000], Loss: 0.1712

Epoch [1/1], Step [59995/60000], Loss: 0.1712

Epoch [1/1], Step [59996/60000], Loss: 0.1712

Epoch [1/1], Step [59997/60000], Loss: 0.1712

Epoch [1/1], Step [59998/60000], Loss: 0.1712

Epoch [1/1], Step [59999/60000], Loss: 0.1712

Epoch [1/1], Step [60000/60000], Loss: 0.1712

Test Accuracy: 98.06 Loss: 0.0628

2021-6-10 10:52:28 : [INFO][SC][53607]: SambaConnector: PEF File:
pef/lenet/lenet.pef

Log ID initialized to: [ALCFUserID][python][53607] at
/var/log/sambaflow/runtime/sn.log
```

Please note that there is no measure-performance command handled
currently in lenet.py and some of the other files in the pytorch starter
code. Hence if you run measure-performance on lenet.py, it will not
display anything. One way to run performance measure for lenet is, add
the following in the main function:

```text
elif args.command == "measure-performance":
        common_app_driver(args, model, inputs, optimizer, name='ffn_mnist_torch', app_dir=utils.get_file_dir(__file__))
```

## LeNet3D

Change directory (if necessary)

```bash
cd ~/apps/starters/
```

LeNet3D does compile but its "run" command is missing data at this
time.

```bash
srun python lenet3d.py compile --pef-name=lenet3d --output-folder="pef"
srun python lenet3d.py test -p pef/lenet3d/lenet3d.pef
srun python lenet3d.py measure-performance -p pef/lenet3d/lenet3d.pef
```

## MNIST - Feed Forward Network

Change directory (if necessary)

```bash
cd ~/apps/starters/
```

Copy data files:

```bash
cp -r /software/sambanova/dataset/mnist_data/ .
```

### MNIST Arguments

This is not an exhaustive list of arguments.

Arguments

| Argument               | Default      | Help                           |
|------------------------|--------------|--------------------------------|
| --lr                   | 0.001        | Learning rate for training     |
|                        |              |                                |
| --momentum             | 0.0          | Momentum value for training    |
|                        |              |                                |
| --weight-decay         | 1e-4         | Weight decay for training      |
|                        |              |                                |

Run these commands:

```bash
srun python ffn_mnist.py compile --pef-name="ffn_mnist" --output-folder="pef"
srun python ffn_mnist.py test --pef="pef/ffn_mnist/ffn_mnist.pef"
srun python ffn_mnist.py run --pef="pef/ffn_mnist/ffn_mnist.pef" --data-path=mnist_data
srun python ffn_mnist.py measure-performance --pef="pef/ffn_mnist/ffn_mnist.pef"
```

To use Slurm, create submit-ffn_mnist-job.sh with the following
contents:

```bash
!/bin/sh

python ffn_mnist.py compile --pef-name="ffn_mnist" --output-folder="pef"
python ffn_mnist.py test --pef="pef/ffn_mnist/ffn_mnist.pef"
python ffn_mnist.py run --pef="pef/ffn_mnist/ffn_mnist.pef" --data-path=mnist_data
python ffn_mnist.py measure-performance --pef="pef/ffn_mnist/ffn_mnist.pef"
```

Then

```bash
sbatch --output=pef/ffn_mnist/output.log submit-ffn_mnist-job.sh
```

Squeue will give you the queue status.

```bash
squeue
```

The output file, pef/ffn_mnist/output.log, will look something like:

```text
pef/ffn_mnist/output.log

[Info][SAMBA][Default] # Placing log files in
pef/ffn_mnist/ffn_mnist.samba.log
[Info][MAC][Default] # Placing log files in
pef/ffn_mnist/ffn_mnist.mac.log
[Warning][SAMBA][Default] #
--------------------------------------------------
Using patched version of torch.cat and torch.stack
--------------------------------------------------

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen
[Warning][MAC][MemoryOpTransformPass] # Backward graph is trimmed
according to requires_grad to save computation.
[Warning][MAC][WeightShareNodeMergePass] # Backward graph is
trimmed according to requires_grad to save computation.
[Warning][MAC][ReduceCatFaninPass] # Backward graph is trimmed
according to requires_grad to save computation.
[info ] [PLASMA] Launching plasma compilation! See log file:
/lambda_stor/homes/ALCFUserID/apps/starters/pytorch/pef/ffn_mnist//ffn_mnist.plasma_compile.log
Prism report not available
Pef file
/lambda_stor/homes/ALCFUserID/apps/starters/pytorch/pef/ffn_mnist/ffn_mnist.pef
created
[info] Run A Round of PerOp Pass: iter 0
[info] We run 1 iterations of PerOp transformation totally.
[info] Compilation succeeded.
[Warning][SAMBA][Default] #
--------------------------------------------------
Using patched version of torch.cat and torch.stack
--------------------------------------------------

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen
samba: tensor(1.2188)
samba.SambaTensor(name:
ffnlogreg__logreg__criterion__crossentropyloss__outputs__0,
shape: torch.Size([]), dtype: torch.float32,
batch_dim: None, named_dims: (), keep_data: True, materializer_provided:
False)
gold: tensor(1.2095, grad_fn=<NllLossBackward>)
samba.SambaTensor(name: sambatensor_140554762586704, shape:
torch.Size([]), dtype: torch.float32,
batch_dim: None, named_dims: (), keep_data: True, materializer_provided:
False)
samba: tensor([[ 1.8516, 0.1934, -1.4141, -0.3281, 0.8945, 0.6797,
0.7031, -1.8984,
-0.2090, 1.8984]])
samba.SambaTensor(name:
ffnlogreg__logreg__lin_layer__linear__outputs__0, shape:
torch.Size([1, 10]), dtype: torch.float32,
batch_dim: None, named_dims: (None, None), keep_data: True,
materializer_provided: False)
gold: tensor([[ 1.8645, 0.1935, -1.4266, -0.3330, 0.9037, 0.6794,
0.7048, -1.9099,
-0.2141, 1.9202]], grad_fn=<MmBackward>)
samba.SambaTensor(name: sambatensor_140554762587664, shape:
torch.Size([1, 10]), dtype: torch.float32,
batch_dim: None, named_dims: (None, None), keep_data: True,
materializer_provided: False)
2021-6-10 14:54:47 : [INFO][SC][74527]: SambaConnector: PEF File:
pef/ffn_mnist/ffn_mnist.pef
Log ID initialized to: [ALCFUserID][python][74527] at
/var/log/sambaflow/runtime/sn.log
[Warning][SAMBA][Default] #
--------------------------------------------------
Using patched version of torch.cat and torch.stack
--------------------------------------------------

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen
Epoch [1/1], Step [10000/60000], Loss: 0.5563
Epoch [1/1], Step [20000/60000], Loss: 0.4374
Epoch [1/1], Step [30000/60000], Loss: 0.3832
Epoch [1/1], Step [40000/60000], Loss: 0.3479
Epoch [1/1], Step [50000/60000], Loss: 0.3222
Epoch [1/1], Step [60000/60000], Loss: 0.3028
Test Accuracy: 94.50 Loss: 0.1905
2021-6-10 14:54:49 : [INFO][SC][74554]: SambaConnector: PEF File:
pef/ffn_mnist/ffn_mnist.pef
Log ID initialized to: [ALCFUserID][python][74554] at
/var/log/sambaflow/runtime/sn.log
```

## MNIST - Feed Forward Network with residual connection.

Feed Forward Network with two different activation functions and a
residual connection.

Change directory (if necessary)

```bash
cd ~/apps/starters/
```

Copy data files:

```bash
cp -r /software/sambanova/dataset/mnist_data/ .
```

Run these commands:

```bash
srun python res_ffn_mnist.py compile --pef-name="res_ffn_mnist" --output-folder="pef"
srun python res_ffn_mnist.py test --pef="pef/res_ffn_mnist/res_ffn_mnist.pef"
srun python res_ffn_mnist.py run --pef="pef/res_ffn_mnist/res_ffn_mnist.pef" --num-workers 1 --data-path=mnist_data
srun python res_ffn_mnist.py measure-performance --pef="pef/res_ffn_mnist/res_ffn_mnist.pef"
```

To use Slurm, create submit-res_ffn_mnist-job.sh with the following
contents:

```bash
!/bin/sh

python res_ffn_mnist.py compile --pef-name="res_ffn_mnist" --output-folder="pef"
python res_ffn_mnist.py test --pef="pef/res_ffn_mnist/res_ffn_mnist.pef"
python res_ffn_mnist.py run --pef="pef/res_ffn_mnist/res_ffn_mnist.pef" --num-workers 1 --data-path=mnist_data
python res_ffn_mnist.py measure-performance --pef="pef/res_ffn_mnist/res_ffn_mnist.pef"
```

Then

```bash
sbatch --output=pef/res_ffn_mnist/output.log
submit-res_ffn_mnist-job.sh
```

Squeue will give you the queue status.

```bash
squeue
```

The output file, pef/res_ffn_mnist/output.log, will look something like:

```text
pef/res_ffn_mnist/output.log

[Info][SAMBA][Default] # Placing log files in
pef/res_ffn_mnist/res_ffn_mnist.samba.log
[Info][MAC][Default] # Placing log files in
pef/res_ffn_mnist/res_ffn_mnist.mac.log
[Warning][SAMBA][Default] #
--------------------------------------------------
Using patched version of torch.cat and torch.stack
--------------------------------------------------

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen
[Warning][MAC][MemoryOpTransformPass] # Backward graph is trimmed
according to requires_grad to save computation.
[Warning][MAC][WeightShareNodeMergePass] # Backward graph is
trimmed according to requires_grad to save computation.
[Warning][MAC][ReduceCatFaninPass] # Backward graph is trimmed
according to requires_grad to save computation.
[info ] [PLASMA] Launching plasma compilation! See log file:
/lambda_stor/homes/ALCFUserID/apps/starters/pytorch/pef/res_ffn_mnist//res_ffn_mnist.plasma_compile.log
Prism report not available
Pef file
/lambda_stor/homes/ALCFUserID/apps/starters/pytorch/pef/res_ffn_mnist/res_ffn_mnist.pef
created
[info] Run A Round of PerOp Pass: iter 0
[info] We run 1 iterations of PerOp transformation totally.
[info] Compilation succeeded.

...

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen

Epoch [1/1], Step [10000/60000], Loss: 0.5834
Epoch [1/1], Step [20000/60000], Loss: 0.5559
Epoch [1/1], Step [30000/60000], Loss: 0.5428
Epoch [1/1], Step [40000/60000], Loss: 0.5282
Epoch [1/1], Step [50000/60000], Loss: 0.5181
Epoch [1/1], Step [60000/60000], Loss: 0.5086
Test Accuracy: 90.66 Loss: 0.4340

...

e2e_throughput: 1898.6569130275361 samples/s, measured over 100
iterations. Average latency: 0.0005266880989074707 s.
e2e_latency: 0.05266880989074707 s
2021-6-11 7:58:4 : [INFO][SC][98357]: SambaConnector: PEF File:
pef/res_ffn_mnist/res_ffn_mnist.pef
Log ID initialized to: [ALCFUserID][python][98357] at
/var/log/sambaflow/runtime/sn.log
```

## Logistic Regression

Change directory (if necessary)

```bash
cd ~/apps/starters/
```

### Logistic Regression Arguments

This is not an exhaustive list of arguments.

Arguments

| Argument            | Default     | Help                         | Step     |
|---------------------|-------------|------------------------------|----------|
| --lr                | 0.001       | Learning rate for training   | Compile  |
|                     |             |                              |          |
| --momentum          | 0.0         | Momentum value for training  | Compile  |
|                     |             |                              |          |
| --weight-decay      | 1e-4        | Weight decay for training    | Compile  |
|                     |             |                              |          |
| --num-features      | 784         | Number features for training | Compile  |
|                     |             |                              |          |
| --num-classes       | 10          | Number classes for training  | Compile  |
|                     |             |                              |          |
| --weight-norm       | na          | Enable weight normalization  | Compile  |
|                     |             |                              |          |

Run these commands:

```bash
srun python logreg.py compile --pef-name="logreg" --output-folder="pef"
srun python logreg.py test --pef="pef/logreg/logreg.pef"
srun python logreg.py run --pef="pef/logreg/logreg.pef"
srun python logreg.py measure-performance --pef="pef/logreg/logreg.pef"
```

To use Slurm, create submit-logreg-job.sh with the following contents:

```bash
!/bin/sh

python logreg.py compile --pef-name="logreg" --output-folder="pef"
python logreg.py test --pef="pef/logreg/logreg.pef"
python logreg.py run --pef="pef/logreg/logreg.pef"
python logreg.py measure-performance --pef="pef/logreg/logreg.pef"
```

Then

```bash
sbatch --output=pef/logreg/output.log submit-logreg-job.sh
```

The output file, pef/logreg/output.log, will look something like:

```text
pef/logreg/output.log

[Info][SAMBA][Default] # Placing log files in
pef/logreg/logreg.samba.log
[Info][MAC][Default] # Placing log files in
pef/logreg/logreg.mac.log
[Warning][SAMBA][Default] #
--------------------------------------------------
Using patched version of torch.cat and torch.stack
--------------------------------------------------

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen
[Warning][MAC][MemoryOpTransformPass] # Backward graph is trimmed
according to requires_grad to save computation.
[Warning][MAC][WeightShareNodeMergePass] # Backward graph is
trimmed according to requires_grad to save computation.
[Warning][MAC][ReduceCatFaninPass] # Backward graph is trimmed
according to requires_grad to save computation.
[info ] [PLASMA] Launching plasma compilation! See log file:
/lambda_stor/homes/ALCFUserID/apps/starters/pytorch/pef/logreg//logreg.plasma_compile.log
...

[Warning][SAMBA][Default] # The dtype of "targets" to
CrossEntropyLoss is torch.int64, however only int16 is currently
supported, implicit conversion will happen
Epoch [1/1], Step [10000/60000], Loss: 0.4763
Epoch [1/1], Step [20000/60000], Loss: 0.4185
Epoch [1/1], Step [30000/60000], Loss: 0.3888
Epoch [1/1], Step [40000/60000], Loss: 0.3721
Epoch [1/1], Step [50000/60000], Loss: 0.3590
Epoch [1/1], Step [60000/60000], Loss: 0.3524
Test Accuracy: 90.07 Loss: 0.3361
2021-6-11 8:38:49 : [INFO][SC][99185]: SambaConnector: PEF File:
pef/logreg/logreg.pef
Log ID initialized to: [ALCFUserID][python][99185] at
/var/log/sambaflow/runtime/sn.log
```

## **UNet**

Change directory

```bash
cd ~/apps/image/pytorch/unet
export OUTDIR=~/apps/image/pytorch/unet
export DATADIR=/var/tmp/kaggle_3m/
```

Export the path to the dataset which is required for the training.

Run these commands for training (compile + train):

```bash
srun python unet.py compile --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size 1 --pef-name="unet_train" --output-folder=${OUTDIR}
srun python unet.py run --do-train --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size 1 --data-dir $DATADIR --log-dir ${OUTDIR} --epochs 5 --pef=${OUTDIR}/unet_train/unet_train.pef
```

Run these commands for inference (compile + test +
measure-performance):

```bash
srun python unet.py compile --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size=1 --inference --pef-name=unet_inf --default-par-factors --output-folder=${OUTDIR}
srun python unet.py run --do-train --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size=1 --inference --pef=${OUTDIR}/unet_inf/unet_inf.pef --log-dir mylogs
srun python unet.py measure-performance --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size=1 --inference --pef=${OUTDIR}/unet_inf/unet_inf.pef
```

Using SLURM :To use Slurm, create submit-unet-job.sh with the following
contents:

```bash
!/bin/sh
export OUTDIR=~/apps/image/pytorch/unet
export DATADIR=/var/tmp/kaggle_3m/
python unet.py compile --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size 1 --pef-name="unet_train" --output-folder=${OUTDIR}
python unet.py run --do-train  --in-channels=3  --in-width=32  --in-height=32 --init-features 32 --batch-size=1? --data-dir $DATADIR --log-dir ${OUTDIR}/log_dir_unet32_train --epochs 5 --pef=${OUTDIR}/unet_train/unet_train.pef
python unet.py compile --in-channels=3  --in-width=32 --in-height=32 --init-features 32  --batch-size=1  --inference --pef-name=unet_inf  --default-par-factors --output-folder=${OUTDIR}
python unet.py run --do-train --in-channels=3 --in-width=32 --in-height=32 --init-features 32 --batch-size=1 --inference --pef=${OUTDIR}/unet_inf/unet_inf.pef --log-dir mylogs
python unet.py measure-performance  --in-channels=3 --in-width=32 --in-height=32  --init-features 32  --batch-size=1 --inference  --pef=${OUTDIR}/unet_inf/unet_inf.pef
```

Then

```bash
sbatch submit-unet-job.sh
```
